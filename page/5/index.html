<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-CN">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222">






  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.0.6" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.0.6">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.0.6">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.0.6">


  <link rel="mask-icon" href="/images/logo.svg?v=6.0.6" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '6.0.6',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="我希望时间没有流走，世界还停在过去，昏黄的光还照在旧日的路上，我刚刚走过，恰与你们相识。">
<meta property="og:type" content="website">
<meta property="og:title" content="Dynamic-Soul">
<meta property="og:url" content="http://yoursite.com/page/5/index.html">
<meta property="og:site_name" content="Dynamic-Soul">
<meta property="og:description" content="我希望时间没有流走，世界还停在过去，昏黄的光还照在旧日的路上，我刚刚走过，恰与你们相识。">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Dynamic-Soul">
<meta name="twitter:description" content="我希望时间没有流走，世界还停在过去，昏黄的光还照在旧日的路上，我刚刚走过，恰与你们相识。">






  <link rel="canonical" href="http://yoursite.com/page/5/"/>



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Dynamic-Soul</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <a href="https://github.com/dynamic-soul"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_gray_6d6d6d.png" alt="Fork me on GitHub"></a>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"> 

<div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Dynamic-Soul</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        
          
  <li class="menu-item menu-item-home">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br />首页</a>
</li>

      
        
        
          
  <li class="menu-item menu-item-about">
    <a href="/about/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-user"></i> <br />关于</a>
</li>

      
        
        
          
  <li class="menu-item menu-item-tags">
    <a href="/tags/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />标签</a>
</li>

      
        
        
          
  <li class="menu-item menu-item-categories">
    <a href="/categories/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-th"></i> <br />分类</a>
</li>

      
        
        
          
  <li class="menu-item menu-item-archives">
    <a href="/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />归档</a>
</li>

      

      
    </ul>
  

  

  
</nav>


  



 </div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/07/2018050720/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mr.Q">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dynamic-Soul">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/07/2018050720/" itemprop="url">正则表达式Re模块</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-07T20:16:43+08:00">2018-05-07</time>
            

            
            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/爬虫/" itemprop="url" rel="index"><span itemprop="name">爬虫</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="页面解析和数据提取"><a href="#页面解析和数据提取" class="headerlink" title="页面解析和数据提取"></a>页面解析和数据提取</h1><p>一般来讲对我们而言，需要抓取的是某个网站或者某个应用的内容，提取有用的价值。内容一般分为两部分，非结构化的数据 和 结构化的数据。</p>
<pre><code>非结构化数据：先有数据，再有结构，
结构化数据：先有结构、再有数据
不同类型的数据，我们需要采用不同的方式来处理。
</code></pre><hr>
<h2 id="非结构化的数据处理"><a href="#非结构化的数据处理" class="headerlink" title="非结构化的数据处理"></a>非结构化的数据处理</h2><p><strong>文本、电话号码、邮箱地址</strong></p>
<ul>
<li><p>正则表达式<br><strong>HTML 文件</strong></p>
</li>
<li><p>正则表达式</p>
</li>
<li>XPath</li>
<li><p>CSS选择器</p>
<h2 id="结构化的数据处理"><a href="#结构化的数据处理" class="headerlink" title="结构化的数据处理"></a>结构化的数据处理</h2><p><strong>JSON 文件</strong></p>
</li>
<li><p>JSON Path</p>
</li>
<li><p>转化成Python类型进行操作（json类）<br><strong>XML 文件</strong></p>
</li>
<li><p>转化成Python类型（xmltodict）</p>
</li>
<li>XPath</li>
<li>CSS选择器</li>
<li>正则表达式<h1 id="什么是正则表达式"><a href="#什么是正则表达式" class="headerlink" title="什么是正则表达式"></a>什么是正则表达式</h1></li>
</ul>
<pre><code>正则表达式，又称规则表达式，通常被用来检索、替换那些符合某个模式(规则)的文本。

正则表达式是对字符串操作的一种逻辑公式，就是用事先定义好的一些特定字符、及这些特定字符的组合，组成一个“规则字符串”，这个“规则字符串”用来表达对字符串的一种过滤逻辑。
</code></pre><p>给定一个正则表达式和另一个字符串，我们可以达到如下的目的：</p>
<pre><code>给定的字符串是否符合正则表达式的过滤逻辑（“匹配”）；
通过正则表达式，从文本字符串中获取我们想要的特定部分（“过滤”）。
</code></pre><img src="/2018/05/07/2018050720/1.jpg">
<h1 id="正则表达式匹配规则"><a href="#正则表达式匹配规则" class="headerlink" title="正则表达式匹配规则"></a>正则表达式匹配规则</h1><img src="/2018/05/07/2018050720/2.jpg">
<h1 id="Python-的-re-模块"><a href="#Python-的-re-模块" class="headerlink" title="Python 的 re 模块"></a>Python 的 re 模块</h1><p>在 Python 中，我们可以使用内置的 re 模块来使用正则表达式。</p>
<p>有一点需要特别注意的是，正则表达式使用 对特殊字符进行转义，所以如果我们要使用原始字符串，只需加一个 r 前缀，示例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">r'chuanzhiboke\t\.\tpython'</span></span><br></pre></td></tr></table></figure></p>
<h2 id="re-模块的一般使用步骤如下："><a href="#re-模块的一般使用步骤如下：" class="headerlink" title="re 模块的一般使用步骤如下："></a>re 模块的一般使用步骤如下：</h2><pre><code>1.使用 compile() 函数将正则表达式的字符串形式编译为一个 Pattern 对象

2.通过 Pattern 对象提供的一系列方法对文本进行匹配查找，获得匹配结果，一个 Match 对象。
3.最后使用 Match 对象提供的属性和方法获得信息，根据需要进行其他的操作
</code></pre><h2 id="compile-函数"><a href="#compile-函数" class="headerlink" title="compile 函数"></a>compile 函数</h2><p>compile 函数用于编译正则表达式，生成一个 Pattern 对象，它的一般使用形式如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将正则表达式编译成 Pattern 对象</span></span><br><span class="line">pattern = re.compile(<span class="string">r'\d+'</span>)</span><br></pre></td></tr></table></figure></p>
<p>在上面，我们已将一个正则表达式编译成 Pattern 对象，接下来，我们就可以利用 pattern 的一系列方法对文本进行匹配查找了。</p>
<p>Pattern 对象的一些常用方法主要有：</p>
<ul>
<li>match 方法：从起始位置开始查找，一次匹配</li>
<li>search 方法：从任何位置开始查找，一次匹配</li>
<li>findall 方法：全部匹配，返回列表</li>
<li>finditer 方法：全部匹配，返回迭代器</li>
<li>split 方法：分割字符串，返回列表</li>
<li>sub 方法：替换</li>
</ul>
<h2 id="match-方法"><a href="#match-方法" class="headerlink" title="match 方法"></a>match 方法</h2><p>match 方法用于查找字符串的头部（也可以指定起始位置），它是一次匹配，只要找到了一个匹配的结果就返回，而不是查找所有匹配的结果。它的一般使用形式如下：</p>
<p>match(string[, pos[, endpos]])</p>
<p>其中，string 是待匹配的字符串，pos 和 endpos 是可选参数，指定字符串的起始和终点位置，默认值分别是 0 和 len (字符串长度)。因此，当你不指定 pos 和 endpos 时，match 方法默认匹配字符串的头部。</p>
<p>当匹配成功时，返回一个 Match 对象，如果没有匹配上，则返回 None。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> re</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pattern = re.compile(<span class="string">r'\d+'</span>)  <span class="comment"># 用于匹配至少一个数字</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = pattern.match(<span class="string">'one12twothree34four'</span>)  <span class="comment"># 查找头部，没有匹配</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> m</span><br><span class="line"><span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = pattern.match(<span class="string">'one12twothree34four'</span>, <span class="number">2</span>, <span class="number">10</span>) <span class="comment"># 从'e'的位置开始匹配，没有匹配</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> m</span><br><span class="line"><span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = pattern.match(<span class="string">'one12twothree34four'</span>, <span class="number">3</span>, <span class="number">10</span>) <span class="comment"># 从'1'的位置开始匹配，正好匹配</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> m                                         <span class="comment"># 返回一个 Match 对象</span></span><br><span class="line">&lt;_sre.SRE_Match object at <span class="number">0x10a42aac0</span>&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.group(<span class="number">0</span>)   <span class="comment"># 可省略 0</span></span><br><span class="line"><span class="string">'12'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.start(<span class="number">0</span>)   <span class="comment"># 可省略 0</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.end(<span class="number">0</span>)     <span class="comment"># 可省略 0</span></span><br><span class="line"><span class="number">5</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.span(<span class="number">0</span>)    <span class="comment"># 可省略 0</span></span><br><span class="line">(<span class="number">3</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure></p>
<p>在上面，当匹配成功时返回一个 Match 对象，其中：</p>
<ul>
<li><p>group([group1, …]) 方法用于获得一个或多个分组匹配的字符串，当要获得整个匹配的子串时，可直接使用 group() 或 group(0)；</p>
</li>
<li><p>start([group]) 方法用于获取分组匹配的子串在整个字符串中的起始位置（子串第一个字符的索引），参数默认值为 0；</p>
</li>
<li>end([group]) 方法用于获取分组匹配的子串在整个字符串中的结束位置（子串最后一个字符的索引+1），参数默认值为 0；</li>
<li>span([group]) 方法返回 (start(group), end(group))。</li>
</ul>
<p>再看看一个例子：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> re</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pattern = re.compile(<span class="string">r'([a-z]+) ([a-z]+)'</span>, re.I)  <span class="comment"># re.I 表示忽略大小写</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = pattern.match(<span class="string">'Hello World Wide Web'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> m     <span class="comment"># 匹配成功，返回一个 Match 对象</span></span><br><span class="line">&lt;_sre.SRE_Match object at <span class="number">0x10bea83e8</span>&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.group(<span class="number">0</span>)  <span class="comment"># 返回匹配成功的整个子串</span></span><br><span class="line"><span class="string">'Hello World'</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.span(<span class="number">0</span>)   <span class="comment"># 返回匹配成功的整个子串的索引</span></span><br><span class="line">(<span class="number">0</span>, <span class="number">11</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.group(<span class="number">1</span>)  <span class="comment"># 返回第一个分组匹配成功的子串</span></span><br><span class="line"><span class="string">'Hello'</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.span(<span class="number">1</span>)   <span class="comment"># 返回第一个分组匹配成功的子串的索引</span></span><br><span class="line">(<span class="number">0</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.group(<span class="number">2</span>)  <span class="comment"># 返回第二个分组匹配成功的子串</span></span><br><span class="line"><span class="string">'World'</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.span(<span class="number">2</span>)   <span class="comment"># 返回第二个分组匹配成功的子串</span></span><br><span class="line">(<span class="number">6</span>, <span class="number">11</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.groups()  <span class="comment"># 等价于 (m.group(1), m.group(2), ...)</span></span><br><span class="line">(<span class="string">'Hello'</span>, <span class="string">'World'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.group(<span class="number">3</span>)   <span class="comment"># 不存在第三个分组</span></span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"&lt;stdin&gt;"</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">IndexError: no such group</span><br></pre></td></tr></table></figure></p>
<h2 id="search-方法"><a href="#search-方法" class="headerlink" title="search 方法"></a>search 方法</h2><p>search 方法用于查找字符串的任何位置，它也是一次匹配，只要找到了一个匹配的结果就返回，而不是查找所有匹配的结果，它的一般使用形式如下：</p>
<p>search(string[, pos[, endpos]])</p>
<p>其中，string 是待匹配的字符串，pos 和 endpos 是可选参数，指定字符串的起始和终点位置，默认值分别是 0 和 len (字符串长度)。</p>
<p>当匹配成功时，返回一个 Match 对象，如果没有匹配上，则返回 None。</p>
<p>让我们看看例子：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> re</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pattern = re.compile(<span class="string">'\d+'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = pattern.search(<span class="string">'one12twothree34four'</span>)  <span class="comment"># 这里如果使用 match 方法则不匹配</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m</span><br><span class="line">&lt;_sre.SRE_Match object at <span class="number">0x10cc03ac0</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.group()</span><br><span class="line"><span class="string">'12'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = pattern.search(<span class="string">'one12twothree34four'</span>, <span class="number">10</span>, <span class="number">30</span>)  <span class="comment"># 指定字符串区间</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m</span><br><span class="line">&lt;_sre.SRE_Match object at <span class="number">0x10cc03b28</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.group()</span><br><span class="line"><span class="string">'34'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.span()</span><br><span class="line">(<span class="number">13</span>, <span class="number">15</span>)</span><br></pre></td></tr></table></figure></p>
<p>再来看一个例子：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="comment"># 将正则表达式编译成 Pattern 对象</span></span><br><span class="line">pattern = re.compile(<span class="string">r'\d+'</span>)</span><br><span class="line"><span class="comment"># 使用 search() 查找匹配的子串，不存在匹配的子串时将返回 None</span></span><br><span class="line"><span class="comment"># 这里使用 match() 无法成功匹配</span></span><br><span class="line">m = pattern.search(<span class="string">'hello 123456 789'</span>)</span><br><span class="line"><span class="keyword">if</span> m:</span><br><span class="line">    <span class="comment"># 使用 Match 获得分组信息</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'matching string:'</span>,m.group()</span><br><span class="line">    <span class="comment"># 起始位置和结束位置</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'position:'</span>,m.span()</span><br></pre></td></tr></table></figure></p>
<p>执行结果：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">matching string: <span class="number">123456</span></span><br><span class="line">position: (<span class="number">6</span>, <span class="number">12</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="findall-方法"><a href="#findall-方法" class="headerlink" title="findall 方法"></a>findall 方法</h2><p>上面的 match 和 search 方法都是一次匹配，只要找到了一个匹配的结果就返回。然而，在大多数时候，我们需要搜索整个字符串，获得所有匹配的结果。</p>
<p>findall 方法的使用形式如下：</p>
<p>findall(string[, pos[, endpos]])</p>
<p>其中，string 是待匹配的字符串，pos 和 endpos 是可选参数，指定字符串的起始和终点位置，默认值分别是 0 和 len (字符串长度)。</p>
<p>findall 以列表形式返回全部能匹配的子串，如果没有匹配，则返回一个空列表。</p>
<p>看看例子：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">pattern = re.compile(<span class="string">r'\d+'</span>)   <span class="comment"># 查找数字</span></span><br><span class="line"></span><br><span class="line">result1 = pattern.findall(<span class="string">'hello 123456 789'</span>)</span><br><span class="line">result2 = pattern.findall(<span class="string">'one1two2three3four4'</span>, <span class="number">0</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> result1</span><br><span class="line"><span class="keyword">print</span> result2</span><br></pre></td></tr></table></figure></p>
<p>执行结果：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'123456'</span>, <span class="string">'789'</span>]</span><br><span class="line">[<span class="string">'1'</span>, <span class="string">'2'</span>]</span><br></pre></td></tr></table></figure></p>
<p>再先看一个栗子：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># re_test.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="comment">#re模块提供一个方法叫compile模块，提供我们输入一个匹配的规则</span></span><br><span class="line"><span class="comment">#然后返回一个pattern实例，我们根据这个规则去匹配字符串</span></span><br><span class="line">pattern = re.compile(<span class="string">r'\d+\.\d*'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#通过partten.findall()方法就能够全部匹配到我们得到的字符串</span></span><br><span class="line">result = pattern.findall(<span class="string">"123.141593, 'bigcat', 232312, 3.15"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#findall 以 列表形式 返回全部能匹配的子串给result</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> result:</span><br><span class="line">    <span class="keyword">print</span> item</span><br></pre></td></tr></table></figure></p>
<p>运行结果：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">123.141593</span></span><br><span class="line"><span class="number">3.15</span></span><br></pre></td></tr></table></figure></p>
<h2 id="finditer-方法"><a href="#finditer-方法" class="headerlink" title="finditer 方法"></a>finditer 方法</h2><p>finditer 方法的行为跟 findall 的行为类似，也是搜索整个字符串，获得所有匹配的结果。但它返回一个顺序访问每一个匹配结果（Match 对象）的迭代器。</p>
<p>看看例子：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line">pattern = re.compile(<span class="string">r'\d+'</span>)</span><br><span class="line"></span><br><span class="line">result_iter1 = pattern.finditer(<span class="string">'hello 123456 789'</span>)</span><br><span class="line">result_iter2 = pattern.finditer(<span class="string">'one1two2three3four4'</span>, <span class="number">0</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> type(result_iter1)</span><br><span class="line"><span class="keyword">print</span> type(result_iter2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> <span class="string">'result1...'</span></span><br><span class="line"><span class="keyword">for</span> m1 <span class="keyword">in</span> result_iter1:   <span class="comment"># m1 是 Match 对象</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'matching string: &#123;&#125;, position: &#123;&#125;'</span>.format(m1.group(), m1.span())</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> <span class="string">'result2...'</span></span><br><span class="line"><span class="keyword">for</span> m2 <span class="keyword">in</span> result_iter2:</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'matching string: &#123;&#125;, position: &#123;&#125;'</span>.format(m2.group(), m2.span())</span><br></pre></td></tr></table></figure></p>
<p>执行结果：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;type <span class="string">'callable-iterator'</span>&gt;</span><br><span class="line">&lt;type <span class="string">'callable-iterator'</span>&gt;</span><br><span class="line">result1...</span><br><span class="line">matching string: <span class="number">123456</span>, position: (<span class="number">6</span>, <span class="number">12</span>)</span><br><span class="line">matching string: <span class="number">789</span>, position: (<span class="number">13</span>, <span class="number">16</span>)</span><br><span class="line">result2...</span><br><span class="line">matching string: <span class="number">1</span>, position: (<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">matching string: <span class="number">2</span>, position: (<span class="number">7</span>, <span class="number">8</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="split-方法"><a href="#split-方法" class="headerlink" title="split 方法"></a>split 方法</h2><p>split 方法按照能够匹配的子串将字符串分割后返回列表，它的使用形式如下：</p>
<p>split(string[, maxsplit])</p>
<p>其中，maxsplit 用于指定最大分割次数，不指定将全部分割。</p>
<p>看看例子：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">p = re.compile(<span class="string">r'[\s\,\;]+'</span>)</span><br><span class="line"><span class="keyword">print</span> p.split(<span class="string">'a,b;; c   d'</span>)</span><br></pre></td></tr></table></figure></p>
<p>执行结果：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>]</span><br></pre></td></tr></table></figure></p>
<h2 id="sub-方法"><a href="#sub-方法" class="headerlink" title="sub 方法"></a>sub 方法</h2><p>sub 方法用于替换。它的使用形式如下：</p>
<p>sub(repl, string[, count])</p>
<p>其中，repl 可以是字符串也可以是一个函数：</p>
<ul>
<li><p>如果 repl 是字符串，则会使用 repl 去替换字符串每一个匹配的子串，并返回替换后的字符串，另外，repl 还可以使用 id 的形式来引用分组，但不能使用编号 0；</p>
</li>
<li><p>如果 repl 是函数，这个方法应当只接受一个参数（Match 对象），并返回一个字符串用于替换（返回的字符串中不能再引用分组）。</p>
</li>
<li>count 用于指定最多替换次数，不指定时全部替换。</li>
</ul>
<p>看看例子：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">p = re.compile(<span class="string">r'(\w+) (\w+)'</span>) <span class="comment"># \w = [A-Za-z0-9]</span></span><br><span class="line">s = <span class="string">'hello 123, hello 456'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> p.sub(<span class="string">r'hello world'</span>, s)  <span class="comment"># 使用 'hello world' 替换 'hello 123' 和 'hello 456'</span></span><br><span class="line"><span class="keyword">print</span> p.sub(<span class="string">r'\2 \1'</span>, s)        <span class="comment"># 引用分组</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(m)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'hi'</span> + <span class="string">' '</span> + m.group(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> p.sub(func, s)</span><br><span class="line"><span class="keyword">print</span> p.sub(func, s, <span class="number">1</span>)         <span class="comment"># 最多替换一次</span></span><br></pre></td></tr></table></figure></p>
<p>执行结果：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hello world, hello world</span><br><span class="line"><span class="number">123</span> hello, <span class="number">456</span> hello</span><br><span class="line">hi <span class="number">123</span>, hi <span class="number">456</span></span><br><span class="line">hi <span class="number">123</span>, hello <span class="number">456</span></span><br></pre></td></tr></table></figure></p>
<h2 id="匹配中文"><a href="#匹配中文" class="headerlink" title="匹配中文"></a>匹配中文</h2><p>在某些情况下，我们想匹配文本中的汉字，有一点需要注意的是，中文的 unicode 编码范围 主要在 [u4e00-u9fa5]，这里说主要是因为这个范围并不完整，比如没有包括全角（中文）标点，不过，在大部分情况下，应该是够用的。</p>
<p>假设现在想把字符串 title = u’你好，hello，世界’ 中的中文提取出来，可以这么做：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">title = <span class="string">u'你好，hello，世界'</span></span><br><span class="line">pattern = re.compile(<span class="string">ur'[\u4e00-\u9fa5]+'</span>)</span><br><span class="line">result = pattern.findall(title)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> result</span><br></pre></td></tr></table></figure></p>
<p>执行结果:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">u'\u4f60\u597d'</span>, <span class="string">u'\u4e16\u754c'</span>]</span><br></pre></td></tr></table></figure></p>
<h2 id="注意：贪婪模式与非贪婪模式"><a href="#注意：贪婪模式与非贪婪模式" class="headerlink" title="注意：贪婪模式与非贪婪模式"></a>注意：贪婪模式与非贪婪模式</h2><p>1.贪婪模式：在整个表达式匹配成功的前提下，尽可能多的匹配 ( * )；<br>2.非贪婪模式：在整个表达式匹配成功的前提下，尽可能少的匹配 ( ? )；<br><strong>3.Python里数量词默认是贪婪的。</strong></p>
<h3 id="示例一-：-源字符串：abbbc"><a href="#示例一-：-源字符串：abbbc" class="headerlink" title="示例一 ： 源字符串：abbbc"></a>示例一 ： 源字符串：abbbc</h3><ul>
<li><p>使用贪婪的数量词的正则表达式 ab* ，匹配结果： abbb。</p>
<ul>
<li>决定了尽可能多匹配 b，所以a后面所有的 b 都出现了。</li>
</ul>
</li>
<li><p>使用非贪婪的数量词的正则表达式ab*?，匹配结果： a。</p>
<p>   即使前面有 *，但是 ? 决定了尽可能少匹配 b，所以没有 b。</p>
</li>
</ul>
<h3 id="示例二-：-源字符串：aa-lt-div-test1-lt-div-bb-lt-div-test2-lt-div-cc"><a href="#示例二-：-源字符串：aa-lt-div-test1-lt-div-bb-lt-div-test2-lt-div-cc" class="headerlink" title="示例二 ： 源字符串：aa&lt;div>test1&lt;/div>bb&lt;div>test2&lt;/div>cc"></a>示例二 ： 源字符串：aa&lt;div>test1&lt;/div>bb&lt;div>test2&lt;/div>cc</h3><ul>
<li><p>使用贪婪的数量词的正则表达式：&lt;div>.*&lt;/div></p>
</li>
<li><p>匹配结果：&lt;div>test1&lt;/div>bb&lt;div>test2&lt;/div></p>
<p>这里采用的是贪婪模式。在匹配到第一个“&lt;/div>”时已经可以使整个表达式匹配成功，但是由于采用的是贪婪模式，所以仍然要向右尝试匹配，查看是否还有更长的可以成功匹配的子串。匹配到第二个“&lt;/div>”后，向右再没有可以成功匹配的子串，匹配结束，匹配结果为“&lt;div>test1&lt;/div>bb&lt;div>test2&lt;/div>”</p>
</li>
<li><p>使用非贪婪的数量词的正则表达式：&lt;div>.*?&lt;/div></p>
</li>
<li><p>匹配结果：&lt;div>test1&lt;/div></p>
<p>正则表达式二采用的是非贪婪模式，在匹配到第一个“&lt;/div>”时使整个表达式匹配成功，由于采用的是非贪婪模式，所以结束匹配，不再向右尝试，匹配结果为“&lt;div>test1&lt;/div>”。</p>
</li>
</ul>
<p><a href="http://tool.oschina.net/regex/" target="_blank" rel="noopener">正则表达式测试网址</a></p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/06/2018050622/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mr.Q">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dynamic-Soul">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/06/2018050622/" itemprop="url">Request模块</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-06T22:24:45+08:00">2018-05-06</time>
            

            
            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/爬虫/" itemprop="url" rel="index"><span itemprop="name">爬虫</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Requests-让-HTTP-服务人类"><a href="#Requests-让-HTTP-服务人类" class="headerlink" title="Requests: 让 HTTP 服务人类"></a>Requests: 让 HTTP 服务人类</h1><p>虽然Python的标准库中 urllib2 模块已经包含了平常我们使用的大多数功能，但是它的 API 使用起来让人感觉不太好，而 Requests 自称 “HTTP for Humans”，说明使用更简洁方便。</p>
<pre><code>Requests 唯一的一个非转基因的 Python HTTP 库，人类可以安全享用：）
</code></pre><h2 id="Requests-继承了urllib2的所有特性。Requests支持HTTP连接保持和连接池，支持使用cookie保持会话，支持文件上传，支持自动确定响应内容的编码，支持国际化的-URL-和-POST-数据自动编码。"><a href="#Requests-继承了urllib2的所有特性。Requests支持HTTP连接保持和连接池，支持使用cookie保持会话，支持文件上传，支持自动确定响应内容的编码，支持国际化的-URL-和-POST-数据自动编码。" class="headerlink" title="Requests 继承了urllib2的所有特性。Requests支持HTTP连接保持和连接池，支持使用cookie保持会话，支持文件上传，支持自动确定响应内容的编码，支持国际化的 URL 和 POST 数据自动编码。"></a>Requests 继承了urllib2的所有特性。Requests支持HTTP连接保持和连接池，支持使用cookie保持会话，支持文件上传，支持自动确定响应内容的编码，支持国际化的 URL 和 POST 数据自动编码。</h2><h3 id="requests-的底层实现其实就是-urllib3"><a href="#requests-的底层实现其实就是-urllib3" class="headerlink" title="requests 的底层实现其实就是 urllib3"></a>requests 的底层实现其实就是 urllib3</h3><p>Requests的文档非常完备，中文文档也相当不错。Requests能完全满足当前网络的需求，支持Python 2.6—3.5，而且能在PyPy下完美运行。</p>
<p>开源地址：<a href="https://github.com/kennethreitz/requests" target="_blank" rel="noopener">https://github.com/kennethreitz/requests</a></p>
<p>中文文档 API： <a href="http://docs.python-requests.org/zh_CN/latest/index.html" target="_blank" rel="noopener">http://docs.python-requests.org/zh_CN/latest/index.html</a></p>
<h2 id="安装方式"><a href="#安装方式" class="headerlink" title="安装方式"></a>安装方式</h2><p>利用 pip 安装 或者利用 easy_install 都可以完成安装：</p>
<p>$ pip install requests</p>
<p>$ easy_install requests</p>
<h2 id="基本GET请求（headers参数-和-parmas参数）"><a href="#基本GET请求（headers参数-和-parmas参数）" class="headerlink" title="基本GET请求（headers参数 和 parmas参数）"></a>基本GET请求（headers参数 和 parmas参数）</h2><p><strong>1.最基本的GET请求可以直接用get方法</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">response = requests.get(<span class="string">"http://www.baidu.com/"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 也可以这么写</span></span><br><span class="line"><span class="comment"># response = requests.request("get", "http://www.baidu.com/")</span></span><br></pre></td></tr></table></figure></p>
<p><strong>2.添加 headers 和 查询参数</strong><br>如果想添加 headers，可以传入headers参数来增加请求头中的headers信息。如果要将参数放在url中传递，可以利用 params 参数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">kw = &#123;<span class="string">'wd'</span>:<span class="string">'长城'</span>&#125;</span><br><span class="line"></span><br><span class="line">headers = &#123;<span class="string">"User-Agent"</span>: <span class="string">"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36"</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># params 接收一个字典或者字符串的查询参数，字典类型自动转换为url编码，不需要urlencode()</span></span><br><span class="line">response = requests.get(<span class="string">"http://www.baidu.com/s?"</span>, params = kw, headers = headers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看响应内容，response.text 返回的是Unicode格式的数据</span></span><br><span class="line"><span class="keyword">print</span> response.text</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看响应内容，response.content返回的字节流数据</span></span><br><span class="line"><span class="keyword">print</span> respones.content</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看完整url地址</span></span><br><span class="line"><span class="keyword">print</span> response.url</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看响应头部字符编码</span></span><br><span class="line"><span class="keyword">print</span> response.encoding</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看响应码</span></span><br><span class="line"><span class="keyword">print</span> response.status_code</span><br></pre></td></tr></table></figure></p>
<p>运行结果<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">......</span><br><span class="line"></span><br><span class="line">......</span><br><span class="line"></span><br><span class="line"><span class="string">'http://www.baidu.com/s?wd=%E9%95%BF%E5%9F%8E'</span></span><br><span class="line"></span><br><span class="line"><span class="string">'utf-8'</span></span><br><span class="line"></span><br><span class="line"><span class="number">200</span></span><br></pre></td></tr></table></figure></p>
<ul>
<li><p>使用response.text 时，Requests 会基于 HTTP 响应的文本编码自动解码响应内容，大多数 Unicode 字符集都能被无缝地解码。</p>
</li>
<li><p>使用response.content 时，返回的是服务器响应数据的原始二进制字节流，可以用来保存图片等二进制文件。</p>
</li>
</ul>
<h2 id="基本POST请求（data参数）"><a href="#基本POST请求（data参数）" class="headerlink" title="基本POST请求（data参数）"></a>基本POST请求（data参数）</h2><p><strong>1.最基本的GET请求可以直接用post方法</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">response = requests.post(<span class="string">"http://www.baidu.com/"</span>, data = data)</span><br></pre></td></tr></table></figure></p>
<p><strong>2.传入data数据</strong><br>对于 POST 请求来说，我们一般需要为它增加一些参数。那么最基本的传参方法可以利用 data 这个参数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">formdata = &#123;</span><br><span class="line">    <span class="string">"type"</span>:<span class="string">"AUTO"</span>,</span><br><span class="line">    <span class="string">"i"</span>:<span class="string">"i love python"</span>,</span><br><span class="line">    <span class="string">"doctype"</span>:<span class="string">"json"</span>,</span><br><span class="line">    <span class="string">"xmlVersion"</span>:<span class="string">"1.8"</span>,</span><br><span class="line">    <span class="string">"keyfrom"</span>:<span class="string">"fanyi.web"</span>,</span><br><span class="line">    <span class="string">"ue"</span>:<span class="string">"UTF-8"</span>,</span><br><span class="line">    <span class="string">"action"</span>:<span class="string">"FY_BY_ENTER"</span>,</span><br><span class="line">    <span class="string">"typoResult"</span>:<span class="string">"true"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">url = <span class="string">"http://fanyi.youdao.com/translate?smartresult=dict&amp;smartresult=rule&amp;smartresult=ugc&amp;sessionFrom=null"</span></span><br><span class="line"></span><br><span class="line">headers=&#123; <span class="string">"User-Agent"</span>: <span class="string">"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36"</span>&#125;</span><br><span class="line"></span><br><span class="line">response = requests.post(url, data = formdata, headers = headers)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> response.text</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果是json文件可以直接显示</span></span><br><span class="line"><span class="keyword">print</span> response.json()</span><br></pre></td></tr></table></figure></p>
<p>运行结果<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">"type"</span>:<span class="string">"EN2ZH_CN"</span>,<span class="string">"errorCode"</span>:<span class="number">0</span>,<span class="string">"elapsedTime"</span>:<span class="number">2</span>,<span class="string">"translateResult"</span>:[[&#123;<span class="string">"src"</span>:<span class="string">"i love python"</span>,<span class="string">"tgt"</span>:<span class="string">"我喜欢python"</span>&#125;]],<span class="string">"smartResult"</span>:&#123;<span class="string">"type"</span>:<span class="number">1</span>,<span class="string">"entries"</span>:[<span class="string">""</span>,<span class="string">"肆文"</span>,<span class="string">"高德纳"</span>]&#125;&#125;</span><br><span class="line"></span><br><span class="line">&#123;<span class="string">u'errorCode'</span>: <span class="number">0</span>, <span class="string">u'elapsedTime'</span>: <span class="number">0</span>, <span class="string">u'translateResult'</span>: [[&#123;<span class="string">u'src'</span>: <span class="string">u'i love python'</span>, <span class="string">u'tgt'</span>: <span class="string">u'\u6211\u559c\u6b22python'</span>&#125;]], <span class="string">u'smartResult'</span>: &#123;<span class="string">u'type'</span>: <span class="number">1</span>, <span class="string">u'entries'</span>: [<span class="string">u''</span>, <span class="string">u'\u8086\u6587'</span>, <span class="string">u'\u9ad8\u5fb7\u7eb3'</span>]&#125;, <span class="string">u'type'</span>: <span class="string">u'EN2ZH_CN'</span>&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="代理（proxies参数）"><a href="#代理（proxies参数）" class="headerlink" title="代理（proxies参数）"></a>代理（proxies参数）</h2><p>如果需要使用代理，你可以通过为任意请求方法提供 proxies 参数来配置单个请求：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据协议类型，选择不同的代理</span></span><br><span class="line">proxies = &#123;</span><br><span class="line">  <span class="string">"http"</span>: <span class="string">"http://12.34.56.79:9527"</span>,</span><br><span class="line">  <span class="string">"https"</span>: <span class="string">"http://12.34.56.79:9527"</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">response = requests.get(<span class="string">"http://www.baidu.com"</span>, proxies = proxies)</span><br><span class="line"><span class="keyword">print</span> response.text</span><br></pre></td></tr></table></figure></p>
<p>也可以通过本地环境变量 HTTP_PROXY 和 HTTPS_PROXY 来配置代理：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HTTP_PROXY=<span class="string">"http://12.34.56.79:9527"</span></span><br><span class="line">export HTTPS_PROXY=<span class="string">"https://12.34.56.79:9527"</span></span><br></pre></td></tr></table></figure></p>
<h2 id="私密代理验证（特定格式）-和-Web客户端验证（auth-参数）"><a href="#私密代理验证（特定格式）-和-Web客户端验证（auth-参数）" class="headerlink" title="私密代理验证（特定格式） 和 Web客户端验证（auth 参数）"></a>私密代理验证（特定格式） 和 Web客户端验证（auth 参数）</h2><p>urllib2 这里的做法比较复杂，requests只需要一步：</p>
<h3 id="私密代理"><a href="#私密代理" class="headerlink" title="私密代理"></a>私密代理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果代理需要使用HTTP Basic Auth，可以使用下面这种格式：</span></span><br><span class="line">proxy = &#123; <span class="string">"http"</span>: <span class="string">"mr_mao_hacker:sffqry9r@61.158.163.130:16816"</span> &#125;</span><br><span class="line"></span><br><span class="line">response = requests.get(<span class="string">"http://www.baidu.com"</span>, proxies = proxy)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> response.text</span><br></pre></td></tr></table></figure>
<h3 id="web客户端验证"><a href="#web客户端验证" class="headerlink" title="web客户端验证"></a>web客户端验证</h3><p>如果是Web客户端验证，需要添加 auth = (账户名, 密码)<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">auth=(<span class="string">'test'</span>, <span class="string">'123456'</span>)</span><br><span class="line"></span><br><span class="line">response = requests.get(<span class="string">'http://192.168.199.107'</span>, auth = auth)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> response.text</span><br></pre></td></tr></table></figure></p>
<h2 id="Cookies-和-Sission"><a href="#Cookies-和-Sission" class="headerlink" title="Cookies 和 Sission"></a>Cookies 和 Sission</h2><h3 id="Cookies"><a href="#Cookies" class="headerlink" title="Cookies"></a>Cookies</h3><p>如果一个响应中包含了cookie，那么我们可以利用 cookies参数拿到：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">response = requests.get(<span class="string">"http://www.baidu.com/"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 7. 返回CookieJar对象:</span></span><br><span class="line">cookiejar = response.cookies</span><br><span class="line"></span><br><span class="line"><span class="comment"># 8. 将CookieJar转为字典：</span></span><br><span class="line">cookiedict = requests.utils.dict_from_cookiejar(cookiejar)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> cookiejar</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> cookiedict</span><br></pre></td></tr></table></figure></p>
<p>运行结果：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;RequestsCookieJar[&lt;Cookie BDORZ=<span class="number">27315</span> <span class="keyword">for</span> .baidu.com/&gt;]&gt;</span><br><span class="line"></span><br><span class="line">&#123;<span class="string">'BDORZ'</span>: <span class="string">'27315'</span>&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="Sission"><a href="#Sission" class="headerlink" title="Sission"></a>Sission</h3><p>在 requests 里，session对象是一个非常常用的对象，这个对象代表一次用户会话：从客户端浏览器连接服务器开始，到客户端浏览器与服务器断开。</p>
<p>会话能让我们在跨请求时候保持某些参数，比如在同一个 Session 实例发出的所有请求之间保持 cookie 。</p>
<h3 id="实现人人网登录"><a href="#实现人人网登录" class="headerlink" title="实现人人网登录"></a>实现人人网登录</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 创建session对象，可以保存Cookie值</span></span><br><span class="line">ssion = requests.session()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 处理 headers</span></span><br><span class="line">headers = &#123;<span class="string">"User-Agent"</span>: <span class="string">"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36"</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 需要登录的用户名和密码</span></span><br><span class="line">data = &#123;<span class="string">"email"</span>:<span class="string">"mr_mao_hacker@163.com"</span>, <span class="string">"password"</span>:<span class="string">"alarmchime"</span>&#125;  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 发送附带用户名和密码的请求，并获取登录后的Cookie值，保存在ssion里</span></span><br><span class="line">ssion.post(<span class="string">"http://www.renren.com/PLogin.do"</span>, data = data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. ssion包含用户登录后的Cookie值，可以直接访问那些登录后才可以访问的页面</span></span><br><span class="line">response = ssion.get(<span class="string">"http://www.renren.com/410043129/profile"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6. 打印响应内容</span></span><br><span class="line"><span class="keyword">print</span> response.text</span><br></pre></td></tr></table></figure>
<h2 id="处理HTTPS请求-SSL证书验证"><a href="#处理HTTPS请求-SSL证书验证" class="headerlink" title="处理HTTPS请求 SSL证书验证"></a>处理HTTPS请求 SSL证书验证</h2><p>Requests也可以为HTTPS请求验证SSL证书：</p>
<ul>
<li>要想检查某个主机的SSL证书，你可以使用 verify 参数（也可以不写）<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">response = requests.get(<span class="string">"https://www.baidu.com/"</span>, verify=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 也可以省略不写</span></span><br><span class="line"><span class="comment"># response = requests.get("https://www.baidu.com/")</span></span><br><span class="line"><span class="keyword">print</span> r.text</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>运行结果：<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;!--STATUS OK--&gt;&lt;html&gt; &lt;head&gt;&lt;meta http-equiv=content-type content=text/html;charset=utf-8&gt;&lt;meta http-equiv=X-UA-Compatible content=IE=Edge&gt;百度一下，你就知道 ....</span><br></pre></td></tr></table></figure></p>
<ul>
<li>如果SSL证书验证不通过，或者不信任服务器的安全证书，则会报出SSLError，据说 12306 证书是自己做的：<img src="/2018/05/06/2018050622/1.jpg">
来测试一下：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">response = requests.get(<span class="string">"https://www.12306.cn/mormhweb/"</span>)</span><br><span class="line"><span class="keyword">print</span> response.text</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>果然：</p>
<p>SSLError: (“bad handshake: Error([(‘SSL routines’, ‘ssl3_get_server_certificate’, ‘certificate verify failed’)],)”,)</p>
<p>如果我们想跳过 12306 的证书验证，把 verify 设置为 False 就可以正常请求了。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">r = requests.get(<span class="string">"https://www.12306.cn/mormhweb/"</span>, verify = <span class="keyword">False</span>)</span><br></pre></td></tr></table></figure></p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/05/2018050522/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mr.Q">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dynamic-Soul">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/05/2018050522/" itemprop="url">urllib2：URLError与HTTPError</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-05T22:39:08+08:00">2018-05-05</time>
            

            
            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/爬虫/" itemprop="url" rel="index"><span itemprop="name">爬虫</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="urllib2-的异常错误处理"><a href="#urllib2-的异常错误处理" class="headerlink" title="urllib2 的异常错误处理"></a>urllib2 的异常错误处理</h1><p>在我们用urlopen或opener.open方法发出一个请求时，如果urlopen或opener.open不能处理这个response，就产生错误。</p>
<p>这里主要说的是URLError和HTTPError，以及对它们的错误处理。</p>
<hr>
<h2 id="URLError"><a href="#URLError" class="headerlink" title="URLError"></a>URLError</h2><p>URLError 产生的原因主要有：</p>
<pre><code>1. 没有网络连接
2. 服务器连接失败
3. 找不到指定的服务器
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># urllib2_urlerror.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"></span><br><span class="line">requset = urllib2.Request(<span class="string">'http://www.ajkfhafwjqh.com'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    urllib2.urlopen(request, timeout=<span class="number">5</span>)</span><br><span class="line"><span class="keyword">except</span> urllib2.URLError, err:</span><br><span class="line">    <span class="keyword">print</span> err</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<urlopen error="" [errno="" 8]="" nodename="" nor="" servname="" provided,="" or="" not="" known="">

<pre><code>urlopen error，错误代码8，错误原因是没有找到指定的服务器。
</code></pre><h2 id="HTTPError"><a href="#HTTPError" class="headerlink" title="HTTPError"></a>HTTPError</h2><p>HTTPError是URLError的子类，我们发出一个请求时，服务器上都会对应一个response应答对象，其中它包含一个数字”响应状态码”。</p>
<p>如果urlopen或opener.open不能处理的，会产生一个HTTPError，对应相应的状态码，HTTP状态码表示HTTP协议所返回的响应的状态。</p>
<p><strong>注意，urllib2可以为我们处理重定向的页面（也就是3开头的响应码），100-299范围的号码表示成功，所以我们只能看到400-599的错误号码。</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># urllib2_httperror.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"></span><br><span class="line">requset = urllib2.Request(<span class="string">'http://blog.baidu.com/itcast'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    urllib2.urlopen(requset)</span><br><span class="line"><span class="keyword">except</span> urllib2.HTTPError, err:</span><br><span class="line">    <span class="keyword">print</span> err.code</span><br><span class="line">    <span class="keyword">print</span> err</span><br></pre></td></tr></table></figure></p>
<p>运行结果如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">404</span></span><br><span class="line">HTTP Error <span class="number">404</span>: Not Found</span><br></pre></td></tr></table></figure></p>
<pre><code>HTTP Error，错误代号是404，错误原因是Not Found，说明服务器无法找到被请求的页面。

通常产生这种错误的，要么url不对，要么ip被封。
</code></pre><h2 id="改进版"><a href="#改进版" class="headerlink" title="改进版"></a>改进版</h2><p>由于HTTPError的父类是URLError，所以父类的异常应当写到子类异常的后面，所以上述的代码可以这么改写：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># urllib2_botherror.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"></span><br><span class="line">requset = urllib2.Request(<span class="string">'http://blog.baidu.com/itcast'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    urllib2.urlopen(requset)</span><br><span class="line"></span><br><span class="line"><span class="keyword">except</span> urllib2.HTTPError, err:</span><br><span class="line">    <span class="keyword">print</span> err.code</span><br><span class="line"></span><br><span class="line"><span class="keyword">except</span> urllib2.URLError, err:</span><br><span class="line">    <span class="keyword">print</span> err</span><br><span class="line"></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"Good Job"</span></span><br></pre></td></tr></table></figure></p>
<p>运行结果如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">404</span></span><br></pre></td></tr></table></figure></p>
<p><strong>这样我们就可以做到，首先捕获子类的异常，如果子类捕获不到，那么可以捕获父类的异常。</strong></p>
<h1 id="HTTP响应状态码参考："><a href="#HTTP响应状态码参考：" class="headerlink" title="HTTP响应状态码参考："></a>HTTP响应状态码参考：</h1><pre><code>1xx:信息

100 Continue
服务器仅接收到部分请求，但是一旦服务器并没有拒绝该请求，客户端应该继续发送其余的请求。
101 Switching Protocols
服务器转换协议：服务器将遵从客户的请求转换到另外一种协议。



2xx:成功

200 OK
请求成功（其后是对GET和POST请求的应答文档）
201 Created
请求被创建完成，同时新的资源被创建。
202 Accepted
供处理的请求已被接受，但是处理未完成。
203 Non-authoritative Information
文档已经正常地返回，但一些应答头可能不正确，因为使用的是文档的拷贝。
204 No Content
没有新文档。浏览器应该继续显示原来的文档。如果用户定期地刷新页面，而Servlet可以确定用户文档足够新，这个状态代码是很有用的。
205 Reset Content
没有新文档。但浏览器应该重置它所显示的内容。用来强制浏览器清除表单输入内容。
206 Partial Content
客户发送了一个带有Range头的GET请求，服务器完成了它。



3xx:重定向

300 Multiple Choices
多重选择。链接列表。用户可以选择某链接到达目的地。最多允许五个地址。
301 Moved Permanently
所请求的页面已经转移至新的url。
302 Moved Temporarily
所请求的页面已经临时转移至新的url。
303 See Other
所请求的页面可在别的url下被找到。
304 Not Modified
未按预期修改文档。客户端有缓冲的文档并发出了一个条件性的请求（一般是提供If-Modified-Since头表示客户只想比指定日期更新的文档）。服务器告诉客户，原来缓冲的文档还可以继续使用。
305 Use Proxy
客户请求的文档应该通过Location头所指明的代理服务器提取。
306 Unused
此代码被用于前一版本。目前已不再使用，但是代码依然被保留。
307 Temporary Redirect
被请求的页面已经临时移至新的url。



4xx:客户端错误

400 Bad Request
服务器未能理解请求。
401 Unauthorized
被请求的页面需要用户名和密码。
401.1
登录失败。
401.2
服务器配置导致登录失败。
401.3
由于 ACL 对资源的限制而未获得授权。
401.4
筛选器授权失败。
401.5
ISAPI/CGI 应用程序授权失败。
401.7
访问被 Web 服务器上的 URL 授权策略拒绝。这个错误代码为 IIS 6.0 所专用。
402 Payment Required
此代码尚无法使用。
403 Forbidden
对被请求页面的访问被禁止。
403.1
执行访问被禁止。
403.2
读访问被禁止。
403.3
写访问被禁止。
403.4
要求 SSL。
403.5
要求 SSL 128。
403.6
IP 地址被拒绝。
403.7
要求客户端证书。
403.8
站点访问被拒绝。
403.9
用户数过多。
403.10
配置无效。
403.11
密码更改。
403.12
拒绝访问映射表。
403.13
客户端证书被吊销。
403.14
拒绝目录列表。
403.15
超出客户端访问许可。
403.16
客户端证书不受信任或无效。
403.17
客户端证书已过期或尚未生效。
403.18
在当前的应用程序池中不能执行所请求的 URL。这个错误代码为 IIS 6.0 所专用。
403.19
不能为这个应用程序池中的客户端执行 CGI。这个错误代码为 IIS 6.0 所专用。
403.20
Passport 登录失败。这个错误代码为 IIS 6.0 所专用。
404 Not Found
服务器无法找到被请求的页面。
404.0
没有找到文件或目录。
404.1
无法在所请求的端口上访问 Web 站点。
404.2
Web 服务扩展锁定策略阻止本请求。
404.3
MIME 映射策略阻止本请求。
405 Method Not Allowed
请求中指定的方法不被允许。
406 Not Acceptable
服务器生成的响应无法被客户端所接受。
407 Proxy Authentication Required
用户必须首先使用代理服务器进行验证，这样请求才会被处理。
408 Request Timeout
请求超出了服务器的等待时间。
409 Conflict
由于冲突，请求无法被完成。
410 Gone
被请求的页面不可用。
411 Length Required
&quot;Content-Length&quot; 未被定义。如果无此内容，服务器不会接受请求。
412 Precondition Failed
请求中的前提条件被服务器评估为失败。
413 Request Entity Too Large
由于所请求的实体的太大，服务器不会接受请求。
414 Request-url Too Long
由于url太长，服务器不会接受请求。当post请求被转换为带有很长的查询信息的get请求时，就会发生这种情况。
415 Unsupported Media Type
由于媒介类型不被支持，服务器不会接受请求。
416 Requested Range Not Satisfiable
服务器不能满足客户在请求中指定的Range头。
417 Expectation Failed
执行失败。
423
锁定的错误。



5xx:服务器错误

500 Internal Server Error
请求未完成。服务器遇到不可预知的情况。
500.12
应用程序正忙于在 Web 服务器上重新启动。
500.13
Web 服务器太忙。
500.15
不允许直接请求 Global.asa。
500.16
UNC 授权凭据不正确。这个错误代码为 IIS 6.0 所专用。
500.18
URL 授权存储不能打开。这个错误代码为 IIS 6.0 所专用。
500.100
内部 ASP 错误。
501 Not Implemented
请求未完成。服务器不支持所请求的功能。
502 Bad Gateway
请求未完成。服务器从上游服务器收到一个无效的响应。
502.1
CGI 应用程序超时。　·
502.2
CGI 应用程序出错。
503 Service Unavailable
请求未完成。服务器临时过载或当机。
504 Gateway Timeout
网关超时。
505 HTTP Version Not Supported
服务器不支持请求中指明的HTTP协议版本
</code></pre></urlopen>
          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/04/2018050420/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mr.Q">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dynamic-Soul">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/04/2018050420/" itemprop="url">urllib2：Handler处理器和自定义Opener</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-04T21:05:49+08:00">2018-05-04</time>
            

            
            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/爬虫/" itemprop="url" rel="index"><span itemprop="name">爬虫</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Handler处理器-和-自定义Opener"><a href="#Handler处理器-和-自定义Opener" class="headerlink" title="Handler处理器 和 自定义Opener"></a>Handler处理器 和 自定义Opener</h1><ul>
<li><p>opener是 urllib2.OpenerDirector 的实例，我们之前一直都在使用的urlopen，它是一个特殊的opener（也就是模块帮我们构建好的）。</p>
</li>
<li><p>但是基本的urlopen()方法不支持代理、cookie等其他的HTTP/HTTPS高级功能。所以要支持这些功能：<br>   1.使用相关的 Handler处理器 来创建特定功能的处理器对象；<br>   2.然后通过 urllib2.build_opener()方法使用这些处理器对象，创建自定义opener对象；<br>   3.使用自定义的opener对象，调用open()方法发送请求。</p>
</li>
<li><p>如果程序里所有的请求都使用自定义的opener，可以使用urllib2.install_opener() 将自定义的 opener 对象 定义为 全局opener，表示如果之后凡是调用urlopen，都将使用这个opener（根据自己的需求来选择）</p>
</li>
</ul>
<hr>
<h1 id="简单的自定义opener"><a href="#简单的自定义opener" class="headerlink" title="简单的自定义opener()"></a>简单的自定义opener()</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建一个HTTPHandler 处理器对象，支持处理HTTP请求</span></span><br><span class="line">http_handler = urllib2.HTTPHandler()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建一个HTTPHandler 处理器对象，支持处理HTTPS请求</span></span><br><span class="line"><span class="comment"># http_handler = urllib2.HTTPSHandler()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用urllib2.build_opener()方法，创建支持处理HTTP请求的opener对象</span></span><br><span class="line">opener = urllib2.build_opener(http_handler)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建 Request请求</span></span><br><span class="line">request = urllib2.Request(<span class="string">"http://www.baidu.com/"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用自定义opener对象的open()方法，发送request请求</span></span><br><span class="line">response = opener.open(request)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取服务器响应内容</span></span><br><span class="line"><span class="keyword">print</span> response.read()</span><br></pre></td></tr></table></figure>
<pre><code>这种方式发送请求得到的结果，和使用urllib2.urlopen()发送HTTP/HTTPS请求得到的结果是一样的。
</code></pre><p>如果在 HTTPHandler()增加 debuglevel=1参数，还会将 Debug Log 打开，这样程序在执行的时候，会把收包和发包的报头在屏幕上自动打印出来，方便调试，有时可以省去抓包的工作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 仅需要修改的代码部分：</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建一个HTTPHandler 处理器对象，支持处理HTTP请求，同时开启Debug Log，debuglevel 值默认 0</span></span><br><span class="line">http_handler = urllib2.HTTPHandler(debuglevel=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建一个HTTPHSandler 处理器对象，支持处理HTTPS请求，同时开启Debug Log，debuglevel 值默认 0</span></span><br><span class="line">https_handler = urllib2.HTTPSHandler(debuglevel=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h1 id="ProxyHandler处理器（代理设置）"><a href="#ProxyHandler处理器（代理设置）" class="headerlink" title="ProxyHandler处理器（代理设置）"></a>ProxyHandler处理器（代理设置）</h1><p>使用代理IP，这是爬虫/反爬虫的第二大招，通常也是最好用的。</p>
<p>很多网站会检测某一段时间某个IP的访问次数(通过流量统计，系统日志等)，如果访问次数多的不像正常人，它会禁止这个IP的访问。</p>
<p>所以我们可以设置一些代理服务器，每隔一段时间换一个代理，就算IP被禁止，依然可以换个IP继续爬取。</p>
<p>urllib2中通过ProxyHandler来设置使用代理服务器，下面代码说明如何使用自定义opener来使用代理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#urllib2_proxy1.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建了两个代理Handler，一个有代理IP，一个没有代理IP</span></span><br><span class="line">httpproxy_handler = urllib2.ProxyHandler(&#123;<span class="string">"http"</span> : <span class="string">"124.88.67.81:80"</span>&#125;)</span><br><span class="line">nullproxy_handler = urllib2.ProxyHandler(&#123;&#125;)</span><br><span class="line"></span><br><span class="line">proxySwitch = <span class="keyword">True</span> <span class="comment">#定义一个代理开关</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过 urllib2.build_opener()方法使用这些代理Handler对象，创建自定义opener对象</span></span><br><span class="line"><span class="comment"># 根据代理开关是否打开，使用不同的代理模式</span></span><br><span class="line"><span class="keyword">if</span> proxySwitch:  </span><br><span class="line">    opener = urllib2.build_opener(httpproxy_handler)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    opener = urllib2.build_opener(nullproxy_handler)</span><br><span class="line"></span><br><span class="line">request = urllib2.Request(<span class="string">"http://www.baidu.com/"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 如果这么写，只有使用opener.open()方法发送请求才使用自定义的代理，而urlopen()则不使用自定义代理。</span></span><br><span class="line">response = opener.open(request)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 如果这么写，就是将opener应用到全局，之后所有的，不管是opener.open()还是urlopen() 发送请求，都将使用自定义代理。</span></span><br><span class="line"><span class="comment"># urllib2.install_opener(opener)</span></span><br><span class="line"><span class="comment"># response = urlopen(request)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> response.read()</span><br></pre></td></tr></table></figure>
<p>免费的开放代理获取基本没有成本，我们可以在一些代理网站上收集这些免费代理，测试后如果可以用，就把它收集起来用在爬虫上面。</p>
<p>免费短期代理网站举例：</p>
<p><a href="http://www.xicidaili.com/" target="_blank" rel="noopener">西刺免费代理IP</a><br><a href="https://www.kuaidaili.com/free/inha/" target="_blank" rel="noopener">快代理免费代理</a></p>
<p>如果代理IP足够多，就可以像随机获取User-Agent一样，随机选择一个代理去访问网站。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">proxy_list = [</span><br><span class="line">    &#123;<span class="string">"http"</span> : <span class="string">"124.88.67.81:80"</span>&#125;,</span><br><span class="line">    &#123;<span class="string">"http"</span> : <span class="string">"124.88.67.81:80"</span>&#125;,</span><br><span class="line">    &#123;<span class="string">"http"</span> : <span class="string">"124.88.67.81:80"</span>&#125;,</span><br><span class="line">    &#123;<span class="string">"http"</span> : <span class="string">"124.88.67.81:80"</span>&#125;,</span><br><span class="line">    &#123;<span class="string">"http"</span> : <span class="string">"124.88.67.81:80"</span>&#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机选择一个代理</span></span><br><span class="line">proxy = random.choice(proxy_list)</span><br><span class="line"><span class="comment"># 使用选择的代理构建代理处理器对象</span></span><br><span class="line">httpproxy_handler = urllib2.ProxyHandler(proxy)</span><br><span class="line"></span><br><span class="line">opener = urllib2.build_opener(httpproxy_handler)</span><br><span class="line"></span><br><span class="line">request = urllib2.Request(<span class="string">"http://www.baidu.com/"</span>)</span><br><span class="line">response = opener.open(request)</span><br><span class="line"><span class="keyword">print</span> response.read()</span><br></pre></td></tr></table></figure>
   &lt;font=gray&gt; 但是，这些免费开放代理一般会有很多人都在使用，而且代理有寿命短，速度慢，匿名度不高，HTTP/HTTPS支持不稳定等缺点（免费没好货）。

<p>   &lt;font=gray&gt; 所以，专业爬虫工程师或爬虫公司会使用高品质的私密代理，这些代理通常需要找专门的代理供应商购买，再通过用户名/密码授权使用（舍不得孩子套不到狼）。<font></font></p>
<h1 id="HTTPPasswordMgrWithDefaultRealm"><a href="#HTTPPasswordMgrWithDefaultRealm" class="headerlink" title="HTTPPasswordMgrWithDefaultRealm()"></a>HTTPPasswordMgrWithDefaultRealm()</h1><p>HTTPPasswordMgrWithDefaultRealm()类将创建一个密码管理对象，用来保存 HTTP 请求相关的用户名和密码，主要应用两个场景：</p>
<pre><code>1.验证代理授权的用户名和密码 (ProxyBasicAuthHandler())
2.验证Web客户端的的用户名和密码 (HTTPBasicAuthHandler())
</code></pre><h2 id="ProxyBasicAuthHandler-代理授权验证"><a href="#ProxyBasicAuthHandler-代理授权验证" class="headerlink" title="ProxyBasicAuthHandler(代理授权验证)"></a>ProxyBasicAuthHandler(代理授权验证)</h2><p>如果我们使用之前的代码来使用私密代理，会报 HTTP 407 错误，表示代理没有通过身份验证：</p>
<p>urllib2.HTTPError: HTTP Error 407: Proxy Authentication Required</p>
<p>所以我们需要改写代码，通过：</p>
<ul>
<li>HTTPPasswordMgrWithDefaultRealm()：来保存私密代理的用户密码</li>
<li>ProxyBasicAuthHandler()：来处理代理的身份验证。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#urllib2_proxy2.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"></span><br><span class="line"><span class="comment"># 私密代理授权的账户</span></span><br><span class="line">user = <span class="string">"mr_mao_hacker"</span></span><br><span class="line"><span class="comment"># 私密代理授权的密码</span></span><br><span class="line">passwd = <span class="string">"sffqry9r"</span></span><br><span class="line"><span class="comment"># 私密代理 IP</span></span><br><span class="line">proxyserver = <span class="string">"61.158.163.130:16816"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 构建一个密码管理对象，用来保存需要处理的用户名和密码</span></span><br><span class="line">passwdmgr = urllib2.HTTPPasswordMgrWithDefaultRealm()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 添加账户信息，第一个参数realm是与远程服务器相关的域信息，一般没人管它都是写None，后面三个参数分别是 代理服务器、用户名、密码</span></span><br><span class="line">passwdmgr.add_password(<span class="keyword">None</span>, proxyserver, user, passwd)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 构建一个代理基础用户名/密码验证的ProxyBasicAuthHandler处理器对象，参数是创建的密码管理对象</span></span><br><span class="line"><span class="comment">#   注意，这里不再使用普通ProxyHandler类了</span></span><br><span class="line">proxyauth_handler = urllib2.ProxyBasicAuthHandler(passwdmgr)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 通过 build_opener()方法使用这些代理Handler对象，创建自定义opener对象，参数包括构建的 proxy_handler 和 proxyauth_handler</span></span><br><span class="line">opener = urllib2.build_opener(proxyauth_handler)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 构造Request 请求</span></span><br><span class="line">request = urllib2.Request(<span class="string">"http://www.baidu.com/"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6. 使用自定义opener发送请求</span></span><br><span class="line">response = opener.open(request)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 7. 打印响应内容</span></span><br><span class="line"><span class="keyword">print</span> response.read()</span><br></pre></td></tr></table></figure>
<h2 id="HTTPBasicAuthHandler处理器（Web客户端授权验证）"><a href="#HTTPBasicAuthHandler处理器（Web客户端授权验证）" class="headerlink" title="HTTPBasicAuthHandler处理器（Web客户端授权验证）"></a>HTTPBasicAuthHandler处理器（Web客户端授权验证）</h2><img src="/2018/05/04/2018050420/1.jpg">
<p>有些Web服务器（包括HTTP/FTP等）访问时，需要进行用户身份验证，爬虫直接访问会报HTTP 401 错误，表示访问身份未经授权：<br>urllib2.HTTPError: HTTP Error 401: Unauthorized<br>如果我们有客户端的用户名和密码，我们可以通过下面的方法去访问爬取：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用户名</span></span><br><span class="line">user = <span class="string">"test"</span></span><br><span class="line"><span class="comment"># 密码</span></span><br><span class="line">passwd = <span class="string">"123456"</span></span><br><span class="line"><span class="comment"># Web服务器 IP</span></span><br><span class="line">webserver = <span class="string">"http://192.168.199.107"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 构建一个密码管理对象，用来保存需要处理的用户名和密码</span></span><br><span class="line">passwdmgr = urllib2.HTTPPasswordMgrWithDefaultRealm()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 添加账户信息，第一个参数realm是与远程服务器相关的域信息，一般没人管它都是写None，后面三个参数分别是 Web服务器、用户名、密码</span></span><br><span class="line">passwdmgr.add_password(<span class="keyword">None</span>, webserver, user, passwd)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 构建一个HTTP基础用户名/密码验证的HTTPBasicAuthHandler处理器对象，参数是创建的密码管理对象</span></span><br><span class="line">httpauth_handler = urllib2.HTTPBasicAuthHandler(passwdmgr)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 通过 build_opener()方法使用这些代理Handler对象，创建自定义opener对象，参数包括构建的 proxy_handler</span></span><br><span class="line">opener = urllib2.build_opener(httpauth_handler)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 可以选择通过install_opener()方法定义opener为全局opener</span></span><br><span class="line">urllib2.install_opener(opener)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6. 构建 Request对象</span></span><br><span class="line">request = urllib2.Request(<span class="string">"http://192.168.199.107"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 7. 定义opener为全局opener后，可直接使用urlopen()发送请求</span></span><br><span class="line">response = urllib2.urlopen(request)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 8. 打印响应内容</span></span><br><span class="line"><span class="keyword">print</span> response.read()</span><br></pre></td></tr></table></figure></p>
<h1 id="Cookie"><a href="#Cookie" class="headerlink" title="Cookie"></a>Cookie</h1><p>Cookie 是指某些网站服务器为了辨别用户身份和进行Session跟踪，而储存在用户浏览器上的文本文件，Cookie可以保持登录信息到用户下次与服务器的会话。</p>
<h2 id="Cookie原理"><a href="#Cookie原理" class="headerlink" title="Cookie原理"></a>Cookie原理</h2><p>HTTP是无状态的面向连接的协议, 为了保持连接状态, 引入了Cookie机制 Cookie是http消息头中的一种属性，包括：<br>Cookie名字（Name）<br>Cookie的值（Value）<br>Cookie的过期时间（Expires/Max-Age）<br>Cookie作用路径（Path）<br>Cookie所在域名（Domain），<br>使用Cookie进行安全连接（Secure）。</p>
<p>前两个参数是Cookie应用的必要条件，另外，还包括Cookie大小（Size，不同浏览器对Cookie个数及大小限制是有差异的）。</p>
<p>Cookie由变量名和值组成，根据 Netscape公司的规定，Cookie格式如下：</p>
<p>Set－Cookie: NAME=VALUE；Expires=DATE；Path=PATH；Domain=DOMAIN_NAME；SECURE</p>
<h2 id="Cookie应用"><a href="#Cookie应用" class="headerlink" title="Cookie应用"></a>Cookie应用</h2><p>Cookies在爬虫方面最典型的应用是判定注册用户是否已经登录网站，用户可能会得到提示，是否在下一次进入此网站时保留用户信息以便简化登录手续。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取一个有登录信息的Cookie模拟登陆</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 构建一个已经登录过的用户的headers信息</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">"Host"</span>:<span class="string">"www.renren.com"</span>,</span><br><span class="line">    <span class="string">"Connection"</span>:<span class="string">"keep-alive"</span>,</span><br><span class="line">    <span class="string">"Upgrade-Insecure-Requests"</span>:<span class="string">"1"</span>,</span><br><span class="line">    <span class="string">"User-Agent"</span>:<span class="string">"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36"</span>,</span><br><span class="line">    <span class="string">"Accept"</span>:<span class="string">"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8"</span>,</span><br><span class="line">    <span class="string">"Accept-Language"</span>:<span class="string">"zh-CN,zh;q=0.8,en;q=0.6"</span>,</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 便于终端阅读，表示不支持压缩文件</span></span><br><span class="line">    <span class="comment"># Accept-Encoding: gzip, deflate, sdch,</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 重点：这个Cookie是保存了密码无需重复登录的用户的Cookie，这个Cookie里记录了用户名，密码(通常经过RAS加密)</span></span><br><span class="line">    <span class="string">"Cookie"</span>: <span class="string">"anonymid=ixrna3fysufnwv; depovince=GW; _r01_=1; JSESSIONID=abcmaDhEdqIlM7riy5iMv; jebe_key=f6fb270b-d06d-42e6-8b53-e67c3156aa7e%7Cc13c37f53bca9e1e7132d4b58ce00fa3%7C1484060607478%7C1%7C1484060607173; jebecookies=26fb58d1-cbe7-4fc3-a4ad-592233d1b42e|||||; ick_login=1f2b895d-34c7-4a1d-afb7-d84666fad409; _de=BF09EE3A28DED52E6B65F6A4705D973F1383380866D39FF5; p=99e54330ba9f910b02e6b08058f780479; ap=327550029; first_login_flag=1; ln_uact=mr_mao_hacker@163.com; ln_hurl=http://hdn.xnimg.cn/photos/hdn521/20140529/1055/h_main_9A3Z_e0c300019f6a195a.jpg; t=214ca9a28f70ca6aa0801404dda4f6789; societyguester=214ca9a28f70ca6aa0801404dda4f6789; id=327550029; xnsid=745033c5; ver=7.0; loginfrom=syshome"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 通过headers里的报头信息（主要是Cookie信息），构建Request对象</span></span><br><span class="line">urllib2.Request(<span class="string">"http://www.renren.com/"</span>, headers = headers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 直接访问renren主页，服务器会根据headers报头信息（主要是Cookie信息），判断这是一个已经登录的用户，并返回相应的页面</span></span><br><span class="line">response = urllib2.urlopen(request)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 打印响应内容</span></span><br><span class="line"><span class="keyword">print</span> response.read()</span><br></pre></td></tr></table></figure></p>
<p>但是这样做太过复杂，我们先需要在浏览器登录账户，并且设置保存密码，并且通过抓包才能获取这个Cookie，那有么有更简单方便的方法呢？</p>
<h2 id="cookielib库-和-HTTPCookieProcessor处理器"><a href="#cookielib库-和-HTTPCookieProcessor处理器" class="headerlink" title="cookielib库 和 HTTPCookieProcessor处理器"></a>cookielib库 和 HTTPCookieProcessor处理器</h2><p>在Python处理Cookie，一般是通过cookielib模块和 urllib2模块的HTTPCookieProcessor处理器类一起使用。</p>
<pre><code>cookielib模块：主要作用是提供用于存储cookie的对象

HTTPCookieProcessor处理器：主要作用是处理这些cookie对象，并构建handler对象。
</code></pre><h3 id="cookielib-库"><a href="#cookielib-库" class="headerlink" title="cookielib 库"></a>cookielib 库</h3><p>该模块主要的对象有CookieJar、FileCookieJar、MozillaCookieJar、LWPCookieJar。</p>
<blockquote>
<ul>
<li>CookieJar：管理HTTP cookie值、存储HTTP请求生成的cookie、向传出的HTTP请求添加cookie的对象。整个cookie都存储在内存中，对CookieJar实例进行垃圾回收后cookie也将丢失。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>FileCookieJar (filename,delayload=None,policy=None)：从CookieJar派生而来，用来创建FileCookieJar实例，检索cookie信息并将cookie存储到文件中。filename是存储cookie的文件名。delayload为True时支持延迟访问访问文件，即只有在需要时才读取文件或在文件中存储数据。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>MozillaCookieJar (filename,delayload=None,policy=None)：从FileCookieJar派生而来，创建与Mozilla浏览器 cookies.txt兼容的FileCookieJar实例。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>LWPCookieJar (filename,delayload=None,policy=None)：从FileCookieJar派生而来，创建与libwww-perl标准的 Set-Cookie3 文件格式兼容的FileCookieJar实例。<br><em>其实大多数情况下，我们只用CookieJar()，如果需要和本地文件交互，就用 MozillaCookjar() 或 LWPCookieJar()</em><br>我们来做几个案例：<br><strong>1.获取Cookie，并保存到CookieJar()对象中</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># urllib2_cookielibtest1.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"><span class="keyword">import</span> cookielib</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建一个CookieJar对象实例来保存cookie</span></span><br><span class="line">cookiejar = cookielib.CookieJar()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用HTTPCookieProcessor()来创建cookie处理器对象，参数为CookieJar()对象</span></span><br><span class="line">handler=urllib2.HTTPCookieProcessor(cookiejar)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过 build_opener() 来构建opener</span></span><br><span class="line">opener = urllib2.build_opener(handler)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 以get方法访问页面，访问之后会自动保存cookie到cookiejar中</span></span><br><span class="line">opener.open(<span class="string">"http://www.baidu.com"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 可以按标准格式将保存的Cookie打印出来</span></span><br><span class="line">cookieStr = <span class="string">""</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> cookiejar:</span><br><span class="line">    cookieStr = cookieStr + item.name + <span class="string">"="</span> + item.value + <span class="string">";"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 舍去最后一位的分号</span></span><br><span class="line"><span class="keyword">print</span> cookieStr[:<span class="number">-1</span>]</span><br></pre></td></tr></table></figure>
</li>
</ul>
</blockquote>
<p>我们使用以上方法将Cookie保存到cookiejar对象中，然后打印出了cookie中的值，也就是访问百度首页的Cookie值。</p>
<p>运行结果如下：<br>BAIDUID=4327A58E63A92B73FF7A297FB3B2B4D0:FG=1;BIDUPSID=4327A58E63A92B73FF7A297FB3B2B4D0;H_PS_PSSID=1429_21115_17001_21454_21409_21554_21398;PSTM=1480815736;BDSVRTM=0;BD_HOME=0</p>
<p><strong>2. 访问网站获得cookie，并把获得的cookie保存在cookie文件中</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># urllib2_cookielibtest2.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cookielib</span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存cookie的本地磁盘文件名</span></span><br><span class="line">filename = <span class="string">'cookie.txt'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 声明一个MozillaCookieJar(有save实现)对象实例来保存cookie，之后写入文件</span></span><br><span class="line">cookiejar = cookielib.MozillaCookieJar(filename)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用HTTPCookieProcessor()来创建cookie处理器对象，参数为CookieJar()对象</span></span><br><span class="line">handler = urllib2.HTTPCookieProcessor(cookiejar)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过 build_opener() 来构建opener</span></span><br><span class="line">opener = urllib2.build_opener(handler)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个请求，原理同urllib2的urlopen</span></span><br><span class="line">response = opener.open(<span class="string">"http://www.baidu.com"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存cookie到本地文件</span></span><br><span class="line">cookiejar.save()</span><br></pre></td></tr></table></figure></p>
<p><strong>3. 从文件中获取cookies，做为请求的一部分去访问</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># urllib2_cookielibtest2.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cookielib</span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建MozillaCookieJar(有load实现)实例对象</span></span><br><span class="line">cookiejar = cookielib.MozillaCookieJar()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从文件中读取cookie内容到变量</span></span><br><span class="line">cookie.load(<span class="string">'cookie.txt'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用HTTPCookieProcessor()来创建cookie处理器对象，参数为CookieJar()对象</span></span><br><span class="line">handler = urllib2.HTTPCookieProcessor(cookiejar)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过 build_opener() 来构建opener</span></span><br><span class="line">opener = urllib2.build_opener(handler)</span><br><span class="line"></span><br><span class="line">response = opener.open(<span class="string">"http://www.baidu.com"</span>)</span><br></pre></td></tr></table></figure>
<p>模拟登录要注意几点：</p>
<pre><code>1.登录一般都会先有一个HTTP GET，用于拉取一些信息及获得Cookie，然后再HTTP POST登录。
2.HTTP POST登录的链接有可能是动态的，从GET返回的信息中获取。
3.password 有些是明文发送，有些是加密后发送。有些网站甚至采用动态加密的，同时包括了很多其他数据的加密信息，只能通过查看JS源码获得加密算法，再去破解加密，非常困难。
4.大多数网站的登录整体流程是类似的，可能有些细节不一样，所以不能保证其他网站登录成功。
</code></pre>
          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/03/2018050321/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mr.Q">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dynamic-Soul">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/03/2018050321/" itemprop="url">urllib2：GET请求和POST请求</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-03T21:27:30+08:00">2018-05-03</time>
            

            
            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/爬虫/" itemprop="url" rel="index"><span itemprop="name">爬虫</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="urllib2默认只支持HTTP-HTTPS的GET和POST方法"><a href="#urllib2默认只支持HTTP-HTTPS的GET和POST方法" class="headerlink" title="urllib2默认只支持HTTP/HTTPS的GET和POST方法"></a>urllib2默认只支持HTTP/HTTPS的GET和POST方法</h1><h2 id="urllib-urlencode"><a href="#urllib-urlencode" class="headerlink" title="urllib.urlencode()"></a>urllib.urlencode()</h2><p><strong>urllib 和 urllib2 都是接受URL请求的相关模块，但是提供了不同的功能。两个最显著的不同如下：</strong></p>
<blockquote>
<p><em>urllib 仅可以接受URL，不能创建 设置了headers 的Request 类实例；
</em>但是 urllib 提供 urlencode 方法用来GET查询字符串的产生，而 urllib2 则没有。（这是 urllib 和 urllib2 经常一起使用的主要原因）<br>*编码工作使用urllib的urlencode()函数，帮我们将key:value这样的键值对转换成”key=value”这样的字符串，解码工作可以使用urllib的unquote()函数。（注意，不是urllib2.urlencode() )<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># IPython2 中的测试结果</span></span><br><span class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> urllib</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: word = &#123;<span class="string">"wd"</span> : <span class="string">"炉石传说"</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过urllib.urlencode()方法，将字典键值对按URL编码转换，从而能被web服务器接受。</span></span><br><span class="line">In [<span class="number">3</span>]: urllib.urlencode(word)  </span><br><span class="line">Out[<span class="number">3</span>]: <span class="string">"wd=%e7%82%89%e7%9f%b3%e4%bc%a0%e8%af%b4"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过urllib.unquote()方法，把 URL编码字符串，转换回原先字符串。</span></span><br><span class="line">In [<span class="number">4</span>]: <span class="keyword">print</span> urllib.unquote(<span class="string">"wd=%e7%82%89%e7%9f%b3%e4%bc%a0%e8%af%b4"</span>)</span><br><span class="line">wd=炉石传说</span><br></pre></td></tr></table></figure></p>
</blockquote>
<p><strong>一般HTTP请求提交数据，需要编码成 URL编码格式，然后做为url的一部分，或者作为参数传到Request对象中。</strong></p>
<h1 id="Get方式"><a href="#Get方式" class="headerlink" title="Get方式"></a>Get方式</h1><p>GET请求一般用于向服务器获取数据，比如说，用百度搜索炉石传说：<a href="https://www.baidu.com/s?ie=UTF-8&amp;wd=炉石传说" target="_blank" rel="noopener">https://www.baidu.com/s?ie=UTF-8&amp;wd=炉石传说</a><br>浏览器的url会跳转成:<a href="https://www.baidu.com/s?ie=UTF-8&amp;wd=%E7%82%89%E7%9F%B3%E4%BC%A0%E8%AF%B4" target="_blank" rel="noopener">https://www.baidu.com/s?ie=UTF-8&amp;wd=%E7%82%89%E7%9F%B3%E4%BC%A0%E8%AF%B4</a><br>在其中可以看到在请求部分里，<a href="http://www.baidu.com/s" target="_blank" rel="noopener">http://www.baidu.com/s</a>? 之后出现一个长长的字符串，其中就包含要查询的关键词炉石传说，于是可以尝试用默认的Get方式来发送请求。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># urllib2_get.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> urllib      <span class="comment">#负责url编码处理</span></span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"></span><br><span class="line">url = <span class="string">"http://www.baidu.com/s"</span></span><br><span class="line">word = &#123;<span class="string">"wd"</span>:<span class="string">"炉石传说"</span>&#125;</span><br><span class="line">word = urllib.urlencode(word) <span class="comment">#转换成url编码格式（字符串）</span></span><br><span class="line">newurl = url + <span class="string">"?"</span> + word    <span class="comment"># url首个分隔符就是 ?</span></span><br><span class="line"></span><br><span class="line">headers=&#123; <span class="string">"User-Agent"</span>: <span class="string">"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36"</span>&#125;</span><br><span class="line"></span><br><span class="line">request = urllib2.Request(newurl, headers=headers)</span><br><span class="line"></span><br><span class="line">response = urllib2.urlopen(request)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> response.read()</span><br></pre></td></tr></table></figure></p>
<h1 id="批量爬取贴吧页面数据"><a href="#批量爬取贴吧页面数据" class="headerlink" title="批量爬取贴吧页面数据"></a>批量爬取贴吧页面数据</h1><p>首先创建一个python文件, tiebaSpider.py，要完成的是，输入一个百度贴吧的地址，比如：</p>
<p>百度贴吧LOL吧第一页：<a href="http://tieba.baidu.com/f?kw=lol&amp;ie=utf-8&amp;pn=0" target="_blank" rel="noopener">http://tieba.baidu.com/f?kw=lol&amp;ie=utf-8&amp;pn=0</a></p>
<p>第二页： <a href="http://tieba.baidu.com/f?kw=lol&amp;ie=utf-8&amp;pn=50" target="_blank" rel="noopener">http://tieba.baidu.com/f?kw=lol&amp;ie=utf-8&amp;pn=50</a></p>
<p>第三页： <a href="http://tieba.baidu.com/f?kw=lol&amp;ie=utf-8&amp;pn=100" target="_blank" rel="noopener">http://tieba.baidu.com/f?kw=lol&amp;ie=utf-8&amp;pn=100</a></p>
<p>发现规律了吧，贴吧中每个页面不同之处，就是url最后的pn的值，其余的都是一样的，就可以抓住这个规律。<br>简单写一个小爬虫程序，来爬取百度LOL吧的所有网页。</p>
<ul>
<li>先写一个main，提示用户输入要爬取的贴吧名，并用urllib.urlencode()进行转码，然后组合url，假设是lol吧，那么组合后的url就是：<a href="http://tieba.baidu.com/f?kw=lol" target="_blank" rel="noopener">http://tieba.baidu.com/f?kw=lol</a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模拟 main 函数</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line"></span><br><span class="line">    kw = raw_input(<span class="string">"请输入需要爬取的贴吧:"</span>)</span><br><span class="line">    <span class="comment"># 输入起始页和终止页，str转成int类型</span></span><br><span class="line">    beginPage = int(raw_input(<span class="string">"请输入起始页："</span>))</span><br><span class="line">    endPage = int(raw_input(<span class="string">"请输入终止页："</span>))</span><br><span class="line"></span><br><span class="line">    url = <span class="string">"http://tieba.baidu.com/f?"</span></span><br><span class="line">    key = urllib.urlencode(&#123;<span class="string">"kw"</span> : kw&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 组合后的url示例：http://tieba.baidu.com/f?kw=lol</span></span><br><span class="line">    url = url + key</span><br><span class="line">    tiebaSpider(url, beginPage, endPage)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li><p>接下来，写一个百度贴吧爬虫接口，需要传递3个参数给这个接口， 一个是main里组合的url地址，以及起始页码和终止页码，表示要爬取页码的范围。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tiebaSpider</span><span class="params">(url, beginPage, endPage)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">        作用：负责处理url，分配每个url去发送请求</span></span><br><span class="line"><span class="string">        url：需要处理的第一个url</span></span><br><span class="line"><span class="string">        beginPage: 爬虫执行的起始页面</span></span><br><span class="line"><span class="string">        endPage: 爬虫执行的截止页面</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> range(beginPage, endPage + <span class="number">1</span>):</span><br><span class="line">        pn = (page - <span class="number">1</span>) * <span class="number">50</span></span><br><span class="line"></span><br><span class="line">        filename = <span class="string">"第"</span> + str(page) + <span class="string">"页.html"</span></span><br><span class="line">        <span class="comment"># 组合为完整的 url，并且pn值每次增加50</span></span><br><span class="line">        fullurl = url + <span class="string">"&amp;pn="</span> + str(pn)</span><br><span class="line">        <span class="comment">#print fullurl</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 调用loadPage()发送请求获取HTML页面</span></span><br><span class="line">        html = loadPage(fullurl, filename)</span><br><span class="line">        <span class="comment"># 将获取到的HTML页面写入本地磁盘文件</span></span><br><span class="line">        writeFile(html, filename)</span><br></pre></td></tr></table></figure>
</li>
<li><p>之前已经写出一个爬取一个网页的代码。现在，我们可以将它封装成一个小函数loadPage，供我们使用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadPage</span><span class="params">(url, filename)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">        作用：根据url发送请求，获取服务器响应文件</span></span><br><span class="line"><span class="string">        url：需要爬取的url地址</span></span><br><span class="line"><span class="string">        filename: 文件名</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">"正在下载"</span> + filename</span><br><span class="line"></span><br><span class="line">    headers = &#123;<span class="string">"User-Agent"</span>: <span class="string">"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0;"</span>&#125;</span><br><span class="line"></span><br><span class="line">    request = urllib2.Request(url, headers = headers)</span><br><span class="line">    response = urllib2.urlopen(request)</span><br><span class="line">    <span class="keyword">return</span> response.read()</span><br></pre></td></tr></table></figure>
</li>
<li><p>最后如果希望将爬取到了每页的信息存储在本地磁盘上，我们可以简单写一个存储文件的接口。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">writeFile</span><span class="params">(html, filename)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">        作用：保存服务器响应文件到本地磁盘文件里</span></span><br><span class="line"><span class="string">        html: 服务器响应文件</span></span><br><span class="line"><span class="string">        filename: 本地磁盘文件名</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">"正在存储"</span> + filename</span><br><span class="line">    <span class="keyword">with</span> open(filename, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(html)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"-"</span> * <span class="number">20</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>其实很多网站都是这样的，同类网站下的html页面编号，分别对应网址后的网页序号，只要发现规律就可以批量爬取页面了。</p>
<h1 id="POST方式："><a href="#POST方式：" class="headerlink" title="POST方式："></a>POST方式：</h1><p>上面我们说了Request请求对象的里有data参数，它就是用在POST里的，我们要传送的数据就是这个参数data，data是一个字典，里面要匹配键值对。</p>
<h2 id="有道词典翻译网站："><a href="#有道词典翻译网站：" class="headerlink" title="有道词典翻译网站："></a>有道词典翻译网站：</h2><p>输入测试数据，再通过使用Fiddler观察，其中有一条是POST请求，而向服务器发送的请求数据并不是在url里，那么我们可以试着模拟这个POST请求。<br><img src="/2018/05/03/2018050321/youdaopost.jpg"><br>于是，我们可以尝试用POST方式发送请求。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"></span><br><span class="line"><span class="comment"># POST请求的目标URL</span></span><br><span class="line">url = <span class="string">"http://fanyi.youdao.com/translate?smartresult=dict&amp;smartresult=rule&amp;smartresult=ugc&amp;sessionFrom=null"</span></span><br><span class="line"></span><br><span class="line">headers=&#123;<span class="string">"User-Agent"</span>: <span class="string">"Mozilla...."</span>&#125;</span><br><span class="line"></span><br><span class="line">formdata = &#123;</span><br><span class="line">    <span class="string">"type"</span>:<span class="string">"AUTO"</span>,</span><br><span class="line">    <span class="string">"i"</span>:<span class="string">"i love python"</span>,</span><br><span class="line">    <span class="string">"doctype"</span>:<span class="string">"json"</span>,</span><br><span class="line">    <span class="string">"xmlVersion"</span>:<span class="string">"1.8"</span>,</span><br><span class="line">    <span class="string">"keyfrom"</span>:<span class="string">"fanyi.web"</span>,</span><br><span class="line">    <span class="string">"ue"</span>:<span class="string">"UTF-8"</span>,</span><br><span class="line">    <span class="string">"action"</span>:<span class="string">"FY_BY_ENTER"</span>,</span><br><span class="line">    <span class="string">"typoResult"</span>:<span class="string">"true"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">data = urllib.urlencode(formdata)</span><br><span class="line"></span><br><span class="line">request = urllib2.Request(url, data = data, headers = headers)</span><br><span class="line">response = urllib2.urlopen(request)</span><br><span class="line"><span class="keyword">print</span> response.read()</span><br></pre></td></tr></table></figure></p>
<p><strong>发送POST请求时，需要特别注意headers的一些属性：</strong></p>
<pre><code>Content-Length: 144： 是指发送的表单数据长度为144，也就是字符个数是144个。

X-Requested-With: XMLHttpRequest ：表示Ajax异步请求。

Content-Type: application/x-www-form-urlencoded ： 表示浏览器提交 Web 表单时使用，表单数据会按照 name1=value1&amp;name2=value2 键值对形式进行编码。
</code></pre><h1 id="获取AJAX加载的内容"><a href="#获取AJAX加载的内容" class="headerlink" title="获取AJAX加载的内容"></a>获取AJAX加载的内容</h1><p>有些网页内容使用AJAX加载，只要记得，AJAX一般返回的是JSON,直接对AJAX地址进行post或get，就返回JSON数据了。</p>
<p>“作为一名爬虫工程师，你最需要关注的，是数据的来源”<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"></span><br><span class="line"><span class="comment"># demo1</span></span><br><span class="line"></span><br><span class="line">url = <span class="string">"https://movie.douban.com/j/chart/top_list?type=11&amp;interval_id=100%3A90&amp;action"</span></span><br><span class="line"></span><br><span class="line">headers=&#123;<span class="string">"User-Agent"</span>: <span class="string">"Mozilla...."</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 变动的是这两个参数，从start开始往后显示limit个</span></span><br><span class="line">formdata = &#123;</span><br><span class="line">    <span class="string">'start'</span>:<span class="string">'0'</span>,</span><br><span class="line">    <span class="string">'limit'</span>:<span class="string">'10'</span></span><br><span class="line">&#125;</span><br><span class="line">data = urllib.urlencode(formdata)</span><br><span class="line"></span><br><span class="line">request = urllib2.Request(url, data = data, headers = headers)</span><br><span class="line">response = urllib2.urlopen(request)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> response.read()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># demo2</span></span><br><span class="line"></span><br><span class="line">url = <span class="string">"https://movie.douban.com/j/chart/top_list?"</span></span><br><span class="line">headers=&#123;<span class="string">"User-Agent"</span>: <span class="string">"Mozilla...."</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理所有参数</span></span><br><span class="line">formdata = &#123;</span><br><span class="line">    <span class="string">'type'</span>:<span class="string">'11'</span>,</span><br><span class="line">    <span class="string">'interval_id'</span>:<span class="string">'100:90'</span>,</span><br><span class="line">    <span class="string">'action'</span>:<span class="string">''</span>,</span><br><span class="line">    <span class="string">'start'</span>:<span class="string">'0'</span>,</span><br><span class="line">    <span class="string">'limit'</span>:<span class="string">'10'</span></span><br><span class="line">&#125;</span><br><span class="line">data = urllib.urlencode(formdata)</span><br><span class="line"></span><br><span class="line">request = urllib2.Request(url, data = data, headers = headers)</span><br><span class="line">response = urllib2.urlopen(request)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> response.read()</span><br></pre></td></tr></table></figure></p>
<p><strong>问题：为什么有时候POST也能在URL内看到数据？</strong></p>
<blockquote>
<ul>
<li>GET方式是直接以链接形式访问，链接中包含了所有的参数，服务器端用Request.QueryString获取变量的值。如果包含了密码的话是一种不安全的选择，不过你可以直观地看到自己提交了什么内容。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>POST则不会在网址上显示所有的参数，服务器端用Request.Form获取提交的数据，在Form提交的时候。但是HTML代码里如果不指定 method 属性，则默认为GET请求，Form中提交的数据将会附加在url之后，以?分开与url分开。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>表单数据可以作为 URL 字段（method=”get”）或者 HTTP POST （method=”post”）的方式来发送。比如在下面的HTML代码中，表单数据将因为 （method=”get”） 而附加到 URL 上：<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;form action=<span class="string">"form_action.asp"</span> method=<span class="string">"get"</span>&gt;</span><br><span class="line">    &lt;p&gt;First name: &lt;input type="text" name="fname" /&gt;&lt;/p&gt;</span><br><span class="line">    &lt;p&gt;Last name: &lt;input type="text" name="lname" /&gt;&lt;/p&gt;</span><br><span class="line">    &lt;input type=<span class="string">"submit"</span> value=<span class="string">"Submit"</span> /&gt;</span><br><span class="line">&lt;/form&gt;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</blockquote>
<h2 id="处理HTTPS请求-SSL证书验证"><a href="#处理HTTPS请求-SSL证书验证" class="headerlink" title="处理HTTPS请求 SSL证书验证"></a>处理HTTPS请求 SSL证书验证</h2><p>现在随处可见 https 开头的网站，urllib2可以为 HTTPS 请求验证SSL证书，就像web浏览器一样，如果网站的SSL证书是经过CA认证的，则能够正常访问，如：<a href="https://www.baidu.com/等.." target="_blank" rel="noopener">https://www.baidu.com/等..</a>.</p>
<p>如果SSL证书验证不通过，或者操作系统不信任服务器的安全证书，比如浏览器在访问12306网站如：<a href="https://www.12306.cn/mormhweb/的时候，会警告用户证书不受信任。（据说" target="_blank" rel="noopener">https://www.12306.cn/mormhweb/的时候，会警告用户证书不受信任。（据说</a> 12306 网站证书是自己做的，没有通过CA认证）<br>urllib2在访问的时候则会报出SSLError：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"></span><br><span class="line">url = <span class="string">"https://www.12306.cn/mormhweb/"</span></span><br><span class="line"></span><br><span class="line">headers = &#123;<span class="string">"User-Agent"</span>: <span class="string">"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36"</span>&#125;</span><br><span class="line"></span><br><span class="line">request = urllib2.Request(url, headers = headers)</span><br><span class="line"></span><br><span class="line">response = urllib2.urlopen(request)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> response.read()</span><br></pre></td></tr></table></figure></p>
<p>运行结果：<br>urllib2.URLError: <urlopen error="" [ssl:="" certificate_verify_failed]="" certificate="" verify="" failed="" (_ssl.c:590)=""></urlopen></p>
<p>所以，如果以后遇到这种网站，需要单独处理SSL证书，让程序忽略SSL证书验证错误，即可正常访问。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"><span class="comment"># 1. 导入Python SSL处理模块</span></span><br><span class="line"><span class="keyword">import</span> ssl</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 表示忽略未经核实的SSL证书认证</span></span><br><span class="line">context = ssl._create_unverified_context()</span><br><span class="line"></span><br><span class="line">url = <span class="string">"https://www.12306.cn/mormhweb/"</span></span><br><span class="line"></span><br><span class="line">headers = &#123;<span class="string">"User-Agent"</span>: <span class="string">"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36"</span>&#125;</span><br><span class="line"></span><br><span class="line">request = urllib2.Request(url, headers = headers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 在urlopen()方法里 指明添加 context 参数</span></span><br><span class="line">response = urllib2.urlopen(request, context = context)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> response.read()</span><br></pre></td></tr></table></figure></p>
<h2 id="关于CA"><a href="#关于CA" class="headerlink" title="关于CA"></a>关于CA</h2><p>CA(Certificate Authority)是数字证书认证中心的简称，是指发放、管理、废除数字证书的受信任的第三方机构，如北京数字认证股份有限公司、上海市数字证书认证中心有限公司等…</p>
<p>CA的作用是检查证书持有者身份的合法性，并签发证书，以防证书被伪造或篡改，以及对证书和密钥进行管理。</p>
<p>现实生活中可以用身份证来证明身份， 那么在网络世界里，数字证书就是身份证。和现实生活不同的是，并不是每个上网的用户都有数字证书的，往往只有当一个人需要证明自己的身份的时候才需要用到数字证书。</p>
<p>普通用户一般是不需要，因为网站并不关心是谁访问了网站，现在的网站只关心流量。但是反过来，网站就需要证明自己的身份了。</p>
<p>比如说现在钓鱼网站很多的，比如你想访问的是<a href="http://www.baidu.com，但其实你访问的是www.daibu.com”，所以在提交自己的隐私信息之前需要验证一下网站的身份，要求网站出示数字证书。" target="_blank" rel="noopener">www.baidu.com，但其实你访问的是www.daibu.com”，所以在提交自己的隐私信息之前需要验证一下网站的身份，要求网站出示数字证书。</a></p>
<p>一般正常的网站都会主动出示自己的数字证书，来确保客户端和网站服务器之间的通信数据是加密安全的。</p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/03/2018050313/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mr.Q">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dynamic-Soul">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/03/2018050313/" itemprop="url">urllib2模块的基本使用</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-03T13:29:27+08:00">2018-05-03</time>
            

            
            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/爬虫/" itemprop="url" rel="index"><span itemprop="name">爬虫</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="urllib2库的基本使用"><a href="#urllib2库的基本使用" class="headerlink" title="urllib2库的基本使用"></a>urllib2库的基本使用</h1><hr>
<p>所谓网页抓取，就是把URL地址中指定的网络资源从网络流中读取出来，保存到本地。 在Python中有很多库可以用来抓取网页，先来了解urllib2。</p>
<pre><code>urllib2 是 Python2.7 自带的模块(不需要下载，导入即可使用)

urllib2 官方文档：https://docs.python.org/2/library/urllib2.html

urllib2 源码：https://hg.python.org/cpython/file/2.7/Lib/urllib2.py
</code></pre><p>urllib2 在 python3.x 中被改为urllib.request</p>
<h2 id="urlopen"><a href="#urlopen" class="headerlink" title="urlopen"></a>urlopen</h2><p>先来段代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># urllib2_urlopen.py</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入urllib2 库</span></span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 向指定的url发送请求，并返回服务器响应的类文件对象</span></span><br><span class="line">response = urllib2.urlopen(<span class="string">"http://www.baidu.com"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 类文件对象支持 文件对象的操作方法，如read()方法读取文件全部内容，返回字符串</span></span><br><span class="line">html = response.read()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印字符串</span></span><br><span class="line"><span class="keyword">print</span> html</span><br></pre></td></tr></table></figure></p>
<p>执行写的python代码，将打印结果<br>Power@PowerMac ~$: python urllib2_urlopen.py<br><strong>实际上，如果我们在浏览器上打开百度主页， 右键选择“查看源代码”，你会发现，跟我们刚才打印出来的是一模一样。也就是说，上面的4行代码就已经帮我们把百度的首页的全部代码爬了下来。<br>一个基本的url请求对应的python代码真的非常简单。</strong></p>
<h2 id="Request"><a href="#Request" class="headerlink" title="Request"></a>Request</h2><p>在第一个例子里，urlopen()的参数就是一个url地址；</p>
<p>但是如果需要执行更复杂的操作，比如增加HTTP报头，必须创建一个 Request 实例来作为urlopen()的参数；而需要访问的url地址则作为 Request 实例的参数。</p>
<p>我们编辑urllib2_request.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># urllib2_request.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"></span><br><span class="line"><span class="comment"># url 作为Request()方法的参数，构造并返回一个Request对象</span></span><br><span class="line">request = urllib2.Request(<span class="string">"http://www.baidu.com"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Request对象作为urlopen()方法的参数，发送给服务器并接收响应</span></span><br><span class="line">response = urllib2.urlopen(request)</span><br><span class="line"></span><br><span class="line">html = response.read()</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> html</span><br></pre></td></tr></table></figure>
<p><strong>运行结果是完全一样的：</strong></p>
<pre><code>新建Request实例，除了必须要有 url 参数之外，还可以设置另外两个参数：

   1. data（默认空）：是伴随 url 提交的数据（比如要post的数据），同时 HTTP 请求将从 &quot;GET&quot;方式 改为 &quot;POST&quot;方式。

   2. headers（默认空）：是一个字典，包含了需要发送的HTTP报头的键值对。

这两个参数下面会说到。
</code></pre><p>##　User-Agent<br>但是这样直接用urllib2给一个网站发送请求的话，确实略有些唐突了，就好比，人家每家都有门，你以一个路人的身份直接闯进去显然不是很礼貌。而且有一些站点不喜欢被程序（非人为访问）访问，有可能会拒绝你的访问请求。</p>
<p>但是如果我们用一个合法的身份去请求别人网站，显然人家就是欢迎的，所以我们就应该给我们的这个代码加上一个身份，就是所谓的User-Agent头。</p>
<ul>
<li>浏览器 就是互联网世界上公认被允许的身份，如果我们希望我们的爬虫程序更像一个真实用户，那我们第一步，就是需要伪装成一个被公认的浏览器。用不同的浏览器在发送请求的时候，会有不同的User-Agent头。 urllib2默认的User-Agent头为：Python-urllib/x.y（x和y是Python主版本和次版本号,例如 Python-urllib/2.7）<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#urllib2_useragent.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"></span><br><span class="line">url = <span class="string">"http://www.itcast.cn"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#IE 9.0 的 User-Agent，包含在 ua_header里</span></span><br><span class="line">ua_header = &#123;<span class="string">"User-Agent"</span> : <span class="string">"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0;"</span>&#125; </span><br><span class="line"></span><br><span class="line"><span class="comment">#  url 连同 headers，一起构造Request请求，这个请求将附带 IE9.0 浏览器的User-Agent</span></span><br><span class="line">request = urllib2.Request(url, headers = ua_header)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 向服务器发送这个请求</span></span><br><span class="line">response = urllib2.urlopen(request)</span><br><span class="line"></span><br><span class="line">html = response.read()</span><br><span class="line"><span class="keyword">print</span> html</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>##　添加更多的Header信息</p>
<p>在 HTTP Request 中加入特定的 Header，来构造一个完整的HTTP请求消息。</p>
<pre><code>可以通过调用Request.add_header() 添加/修改一个特定的header 也可以通过调用Request.get_header()来查看已有的header。
</code></pre><ul>
<li>添加一个特定的header</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># urllib2_headers.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"></span><br><span class="line">url = <span class="string">"http://www.itcast.cn"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#IE 9.0 的 User-Agent</span></span><br><span class="line">header = &#123;<span class="string">"User-Agent"</span> : <span class="string">"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0;"</span>&#125; </span><br><span class="line">request = urllib2.Request(url, headers = header)</span><br><span class="line"></span><br><span class="line"><span class="comment">#也可以通过调用Request.add_header() 添加/修改一个特定的header</span></span><br><span class="line">request.add_header(<span class="string">"Connection"</span>, <span class="string">"keep-alive"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 也可以通过调用Request.get_header()来查看header信息</span></span><br><span class="line"><span class="comment"># request.get_header(header_name="Connection")</span></span><br><span class="line"></span><br><span class="line">response = urllib2.urlopen(req)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> response.code     <span class="comment">#可以查看响应状态码</span></span><br><span class="line">html = response.read()</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> html</span><br></pre></td></tr></table></figure>
<ul>
<li>随机添加/修改User-Agent<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># urllib2_add_headers.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">url = <span class="string">"http://www.itcast.cn"</span></span><br><span class="line"></span><br><span class="line">ua_list = [</span><br><span class="line">    <span class="string">"Mozilla/5.0 (Windows NT 6.1; ) Apple.... "</span>,</span><br><span class="line">    <span class="string">"Mozilla/5.0 (X11; CrOS i686 2268.111.0)... "</span>,</span><br><span class="line">    <span class="string">"Mozilla/5.0 (Macintosh; U; PPC Mac OS X.... "</span>,</span><br><span class="line">    <span class="string">"Mozilla/5.0 (Macintosh; Intel Mac OS... "</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">user_agent = random.choice(ua_list)</span><br><span class="line"></span><br><span class="line">request = urllib2.Request(url)</span><br><span class="line"></span><br><span class="line"><span class="comment">#也可以通过调用Request.add_header() 添加/修改一个特定的header</span></span><br><span class="line">request.add_header(<span class="string">"User-Agent"</span>, user_agent)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一个字母大写，后面的全部小写</span></span><br><span class="line">request.get_header(<span class="string">"User-agent"</span>)</span><br><span class="line"></span><br><span class="line">response = urllib2.urlopen(req)</span><br><span class="line"></span><br><span class="line">html = response.read()</span><br><span class="line"><span class="keyword">print</span> html</span><br></pre></td></tr></table></figure></li>
</ul>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/02/2018050213/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mr.Q">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dynamic-Soul">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/02/2018050213/" itemprop="url">HTTP/HTTPS的请求与响应</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-02T13:23:30+08:00">2018-05-02</time>
            

            
            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/爬虫/" itemprop="url" rel="index"><span itemprop="name">爬虫</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <hr>
<h1 id="HTTP和HTTPS"><a href="#HTTP和HTTPS" class="headerlink" title="HTTP和HTTPS"></a>HTTP和HTTPS</h1><hr>
<p>HTTP协议（HyperText Transfer Protocol，超文本传输协议）：是一种发布和接收 HTML页面的方法。</p>
<p>HTTPS（Hypertext Transfer Protocol over Secure Socket Layer）简单讲是HTTP的安全版，在HTTP下加入SSL层。</p>
<p>SSL（Secure Sockets Layer 安全套接层）主要用于Web的安全传输协议，在传输层对网络连接进行加密，保障在Internet上数据传输的安全。</p>
<ul>
<li>HTTP的端口号为80，</li>
<li>HTTPS的端口号为443<h2 id="HTTP工作原理"><a href="#HTTP工作原理" class="headerlink" title="HTTP工作原理"></a>HTTP工作原理</h2>网络爬虫抓取过程可以理解为模拟浏览器操作的过程。<br>浏览器的主要功能是向服务器发出请求，在浏览器窗口中展示您选择的网络资源，HTTP是一套计算机通过网络进行通信的规则。</li>
</ul>
<h1 id="HTTP的请求与响应"><a href="#HTTP的请求与响应" class="headerlink" title="HTTP的请求与响应"></a>HTTP的请求与响应</h1><p>HTTP通信由两部分组成： <strong>客户端请求消息</strong> 与 <strong>服务器响应消息</strong></p>
<p>##浏览器发送HTTP请求的过程：</p>
<pre><code>1.当用户在浏览器的地址栏中输入一个URL并按回车键之后，浏览器会向HTTP服务器发送HTTP请求。HTTP请求主要分为“Get”和“Post”两种方法。

2.当我们在浏览器输入URL http://www.baidu.com 的时候，浏览器发送一个Request请求去获取 http://www.baidu.com 的html文件，服务器把Response文件对象发送回给浏览器。

3.浏览器分析Response中的 HTML，发现其中引用了很多其他文件，比如Images文件，CSS文件，JS文件。 浏览器会自动再次发送Request去获取图片，CSS文件，或者JS文件。

4.当所有的文件都下载成功后，网页会根据HTML语法结构，完整的显示出来了。
</code></pre><p>URL（Uniform / Universal Resource Locator的缩写）：统一资源定位符，是用于完整地描述Internet上网页和其他资源的地址的一种标识方法。</p>
<p>基本格式：scheme://host[:port#]/path/…/[?query-string][#anchor]</p>
<ul>
<li>scheme：协议(例如：http, https, ftp)</li>
<li>host：服务器的IP地址或者域名</li>
<li>port#：服务器的端口（如果是走协议默认端口，缺省端口80）</li>
<li>path：访问资源的路径</li>
<li>query-string：参数，发送给http服务器的数据</li>
<li>anchor：锚（跳转到网页的指定锚点位置）</li>
</ul>
<p>例如：</p>
<p><a href="http://www.baidu.com" target="_blank" rel="noopener">http://www.baidu.com</a></p>
<p><a href="http://item.jd.com/11936238.html#product-detail" target="_blank" rel="noopener">http://item.jd.com/11936238.html#product-detail</a></p>
<h1 id="客户端HTTP请求"><a href="#客户端HTTP请求" class="headerlink" title="客户端HTTP请求"></a>客户端HTTP请求</h1><p>URL只是标识资源的位置，而HTTP是用来提交和获取资源。客户端发送一个HTTP请求到服务器的请求消息，包括以下格式：</p>
<p><strong>请求行、请求头部、空行、请求数据</strong></p>
<h3 id="一个典型的HTTP请求示例"><a href="#一个典型的HTTP请求示例" class="headerlink" title="一个典型的HTTP请求示例"></a>一个典型的HTTP请求示例</h3><pre><code>GET https://www.baidu.com/ HTTP/1.1
Host: www.baidu.com
Connection: keep-alive
Upgrade-Insecure-Requests: 1
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8
Referer: http://www.baidu.com/
Accept-Encoding: gzip, deflate, sdch, br
Accept-Language: zh-CN,zh;q=0.8,en;q=0.6
Cookie: BAIDUID=04E4001F34EA74AD4601512DD3C41A7B:FG=1; BIDUPSID=04E4001F34EA74AD4601512DD3C41A7B; PSTM=1470329258; MCITY=-343%3A340%3A; BDUSS=nF0MVFiMTVLcUh-Q2MxQ0M3STZGQUZ4N2hBa1FFRkIzUDI3QlBCZjg5cFdOd1pZQVFBQUFBJCQAAAAAAAAAAAEAAADpLvgG0KGyvLrcyfrG-AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFaq3ldWqt5XN; H_PS_PSSID=1447_18240_21105_21386_21454_21409_21554; BD_UPN=12314753; sug=3; sugstore=0; ORIGIN=0; bdime=0; H_PS_645EC=7e2ad3QHl181NSPbFbd7PRUCE1LlufzxrcFmwYin0E6b%2BW8bbTMKHZbDP0g; BDSVRTM=0
</code></pre><h2 id="请求方法"><a href="#请求方法" class="headerlink" title="请求方法"></a><strong>请求方法</strong></h2><pre><code>GET https://www.baidu.com/ HTTP/1.1  
</code></pre><p>根据HTTP标准，HTTP请求可以使用多种请求方法。</p>
<p>HTTP 0.9：只有基本的文本 GET 功能。</p>
<p>HTTP 1.0：完善的请求/响应模型，并将协议补充完整，定义了三种请求方法： GET, POST 和 HEAD方法。</p>
<p>HTTP 1.1：在 1.0 基础上进行更新，新增了五种请求方法：OPTIONS, PUT, DELETE, TRACE 和 CONNECT 方法。</p>
<p>HTTP 2.0（未普及）：请求/响应首部的定义基本没有改变，只是所有首部键必须全部小写，而且请求行要独立为 :method、:scheme、:host、:path这些键值对。<br>|序号|方法|描述|<br>|—|:—:|—|<br>|1|GET|请求指定的页面信息，并返回实体主体。|<br>|2|HEAD|类似于get请求，只不过返回的响应中没有具体的内容，用于获取报头|<br>|3|POST|向指定资源提交数据进行处理请求（例如提交表单或者上传文件），数据被包含在请求体中。POST请求可能会导致新的资源的建立和/或已有资源的修改。|<br>|4|PUT|从客户端向服务器传送的数据取代指定的文档的内容。|<br>|5|DELETE|请求服务器删除指定的页面。|<br>|6|CONNECT|HTTP/1.1协议中预留给能够将连接改为管道方式的代理服务器。|<br>|7|OPTIONS|允许客户端查看服务器的性能。|<br>|8|TRACE|回显服务器收到的请求，主要用于测试或诊断。|</p>
<h3 id="HTTP请求主要分为Get和Post两种方法"><a href="#HTTP请求主要分为Get和Post两种方法" class="headerlink" title="HTTP请求主要分为Get和Post两种方法"></a><strong>HTTP请求主要分为Get和Post两种方法</strong></h3><ul>
<li><p>GET是从服务器上获取数据，POST是向服务器传送数据</p>
</li>
<li><p>GET请求参数显示，都显示在浏览器网址上，HTTP服务器根据该请求所包含URL中的参数来产生响应内容，即“Get”请求的参数是URL的一部分。 例如： <a href="http://www.baidu.com/s?wd=Chinese" target="_blank" rel="noopener">http://www.baidu.com/s?wd=Chinese</a></p>
</li>
<li><p>POST请求参数在请求体当中，消息长度没有限制而且以隐式的方式进行发送，通常用来向HTTP服务器提交量比较大的数据（比如请求中包含许多参数或者文件上传操作等），请求的参数包含在“Content-Type”消息头里，指明该消息体的媒体类型和编码，</p>
</li>
</ul>
<p><strong>注意：避免使用Get方式提交表单，因为有可能会导致安全问题。 比如说在登陆表单中用Get方式，用户输入的用户名和密码将在地址栏中暴露无遗。</strong></p>
<h3 id="常用的请求报头"><a href="#常用的请求报头" class="headerlink" title="常用的请求报头"></a><strong>常用的请求报头</strong></h3><h4 id="1-Host-主机和端口号"><a href="#1-Host-主机和端口号" class="headerlink" title="1. Host (主机和端口号)"></a><strong>1. Host (主机和端口号)</strong></h4><p>Host：对应网址URL中的Web名称和端口号，用于指定被请求资源的Internet主机和端口号，通常属于URL的一部分。</p>
<h4 id="2-Connection-链接类型"><a href="#2-Connection-链接类型" class="headerlink" title="2. Connection (链接类型)"></a><strong>2. Connection (链接类型)</strong></h4><p>Connection：表示客户端与服务连接类型</p>
<ol>
<li><p>Client 发起一个包含 Connection:keep-alive 的请求，HTTP/1.1使用 keep-alive 为默认值。</p>
</li>
<li><p>Server收到请求后：</p>
<ul>
<li>如果 Server 支持 keep-alive，回复一个包含 Connection:keep-alive 的响应，不关闭连接；</li>
<li>如果 Server 不支持 keep-alive，回复一个包含 Connection:close 的响应，关闭连接。</li>
</ul>
</li>
<li><p>如果client收到包含 Connection:keep-alive 的响应，向同一个连接发送下一个请求，直到一方主动关闭连接。</p>
</li>
</ol>
<p><strong>keep-alive在很多情况下能够重用连接，减少资源消耗，缩短响应时间，比如当浏览器需要多个文件时(比如一个HTML文件和相关的图形文件)，不需要每次都去请求建立连接。</strong></p>
<h4 id="3-Upgrade-Insecure-Requests-升级为HTTPS请求"><a href="#3-Upgrade-Insecure-Requests-升级为HTTPS请求" class="headerlink" title="3. Upgrade-Insecure-Requests (升级为HTTPS请求)"></a><strong>3. Upgrade-Insecure-Requests (升级为HTTPS请求)</strong></h4><p>Upgrade-Insecure-Requests：升级不安全的请求，意思是会在加载 http 资源时自动替换成 https 请求，让浏览器不再显示https页面中的http请求警报。<br><strong><em>HTTPS 是以安全为目标的 HTTP 通道，所以在 HTTPS 承载的页面上不允许出现 HTTP 请求，一旦出现就是提示或报错。</em></strong></p>
<h4 id="4-User-Agent-浏览器名称"><a href="#4-User-Agent-浏览器名称" class="headerlink" title="4. User-Agent (浏览器名称)"></a><strong>4. User-Agent (浏览器名称)</strong></h4><p>User-Agent：是客户浏览器的名称。</p>
<h4 id="5-Accept-传输文件类型"><a href="#5-Accept-传输文件类型" class="headerlink" title="5. Accept (传输文件类型)"></a><strong>5. Accept (传输文件类型)</strong></h4><p>Accept：指浏览器或其他客户端可以接受的MIME（Multipurpose Internet Mail Extensions（多用途互联网邮件扩展））文件类型，服务器可以根据它判断并返回适当的文件格式。</p>
<p><strong>举例</strong><br>Accept: <em>/</em>：表示什么都可以接收。</p>
<p>Accept：image/gif：表明客户端希望接受GIF图像格式的资源；</p>
<p>Accept：text/html：表明客户端希望接受html文本。</p>
<p>Accept: text/html, application/xhtml+xml;q=0.9, image/*;q=0.8：表示浏览器支持的 MIME 类型分别是 html文本、xhtml和xml文档、所有的图像格式资源。<br><strong><em>q是权重系数，范围 0 =&lt; q &lt;= 1，q 值越大，请求越倾向于获得其“;”之前的类型表示的内容。若没有指定q值，则默认为1，按从左到右排序顺序；若被赋值为0，则用于表示浏览器不接受此内容类型。  TEXT:用于标准化地表示的文本信息，文本消息可以是多种字符集和或者多种格式的；Application：用于传输应用程序数据或者二进制数据。</em></strong></p>
<h4 id="6-Referer-页面跳转处"><a href="#6-Referer-页面跳转处" class="headerlink" title="6. Referer (页面跳转处)"></a><strong>6. Referer (页面跳转处)</strong></h4><p>Referer：表明产生请求的网页来自于哪个URL，用户是从该 Referer页面访问到当前请求的页面。这个属性可以用来跟踪Web请求来自哪个页面，是从什么网站来的等。</p>
<p>有时候遇到下载某网站图片，需要对应的referer，否则无法下载图片，那是因为人家做了防盗链，原理就是根据referer去判断是否是本网站的地址，如果不是，则拒绝，如果是，就可以下载；</p>
<h4 id="7-Accept-Encoding（文件编解码格式）"><a href="#7-Accept-Encoding（文件编解码格式）" class="headerlink" title="7. Accept-Encoding（文件编解码格式）"></a><strong>7. Accept-Encoding（文件编解码格式）</strong></h4><p>Accept-Encoding：指出浏览器可以接受的编码方式。编码方式不同于文件格式，它是为了压缩文件并加速文件传递速度。浏览器在接收到Web响应之后先解码，然后再检查文件格式，许多情形下这可以减少大量的下载时间。<br><strong>举例：Accept-Encoding:gzip;q=1.0, identity; q=0.5, *;q=0</strong></p>
<p>如果有多个Encoding同时匹配, 按照q值顺序排列，本例中按顺序支持 gzip, identity压缩编码，支持gzip的浏览器会返回经过gzip编码的HTML页面。 <strong>如果请求消息中没有设置这个域服务器假定客户端对各种内容编码都可以接受。</strong></p>
<h4 id="8-Accept-Language（语言种类）"><a href="#8-Accept-Language（语言种类）" class="headerlink" title="8. Accept-Language（语言种类）"></a><strong>8. Accept-Language（语言种类）</strong></h4><p>Accept-Langeuage：指出浏览器可以接受的语言种类，如en或en-us指英语，zh或者zh-cn指中文，当服务器能够提供一种以上的语言版本时要用到。</p>
<h4 id="9-Accept-Charset（字符编码）"><a href="#9-Accept-Charset（字符编码）" class="headerlink" title="9. Accept-Charset（字符编码）"></a><strong>9. Accept-Charset（字符编码）</strong></h4><p>Accept-Charset：指出浏览器可以接受的字符编码。<br><strong>举例：Accept-Charset:iso-8859-1,gb2312,utf-8</strong></p>
<ul>
<li>ISO8859-1：通常叫做Latin-1。Latin-1包括了书写所有西方欧洲语言不可缺少的附加字符，英文浏览器的默认值是ISO-8859-1.</li>
<li>gb2312：标准简体中文字符集;</li>
<li>utf-8：UNICODE 的一种变长字符编码，可以解决多种语言文本显示问题，从而实现应用国际化和本地化。</li>
</ul>
<p><strong>如果在请求消息中没有设置这个域，缺省是任何字符集都可以接受。</strong></p>
<h4 id="10-Cookie-（Cookie）"><a href="#10-Cookie-（Cookie）" class="headerlink" title="10. Cookie （Cookie）"></a><strong>10. Cookie （Cookie）</strong></h4><p>Cookie：浏览器用这个属性向服务器发送Cookie。Cookie是在浏览器中寄存的小型数据体，它可以记载和服务器相关的用户信息，也可以用来实现会话功能。</p>
<h4 id="11-Content-Type-POST数据类型"><a href="#11-Content-Type-POST数据类型" class="headerlink" title="11. Content-Type (POST数据类型)"></a><strong>11. Content-Type (POST数据类型)</strong></h4><p>Content-Type：POST请求里用来表示的内容类型。<br><strong>举例：Content-Type = Text/XML; charset=gb2312：</strong></p>
<p>指明该请求的消息体中包含的是纯文本的XML类型的数据，字符编码采用“gb2312”。</p>
<h3 id="服务端HTTP响应"><a href="#服务端HTTP响应" class="headerlink" title="服务端HTTP响应"></a>服务端HTTP响应</h3><pre><code>HTTP响应也由四个部分组成，分别是： 状态行、消息报头、空行、响应正文
HTTP/1.1 200 OK
Server: Tengine
Connection: keep-alive
Date: Wed, 30 Nov 2016 07:58:21 GMT
Cache-Control: no-cache
Content-Type: text/html;charset=UTF-8
Keep-Alive: timeout=20
Vary: Accept-Encoding
Pragma: no-cache
X-NWS-LOG-UUID: bd27210a-24e5-4740-8f6c-25dbafa9c395
Content-Length: 180945

&lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD XHTML 1.0 Transitional//EN&quot; ....
</code></pre><h3 id="响应状态码"><a href="#响应状态码" class="headerlink" title="响应状态码"></a>响应状态码</h3><p>响应状态代码有三位数字组成，第一个数字定义了响应的类别，且有五种可能取值。</p>
<h4 id="常见状态码："><a href="#常见状态码：" class="headerlink" title="常见状态码："></a>常见状态码：</h4><blockquote>
<ul>
<li>100~199表示服务器成功接收部分请求，要求客户端继续提交其余请求才能完成整个处理过程。</li>
<li>200~299：表示服务器成功接收请求并已完成整个处理过程。常用200（OK 请求成功）。</li>
<li>300~399：为完成请求，客户需进一步细化请求。例如：请求的资源已经移动一个新地址、常用302（所请求的页面已经临时转移至新的url）、307和304（使用缓存资源）。</li>
<li>400~499：客户端的请求有错误，常用404（服务器无法找到被请求的页面）、403（服务器拒绝访问，权限不够）。</li>
<li>500~599：服务器端出现错误，常用500（请求未完成。服务器遇到不可预知的情况）。</li>
</ul>
</blockquote>
<h1 id="Cookie-和-Session："><a href="#Cookie-和-Session：" class="headerlink" title="Cookie 和 Session："></a>Cookie 和 Session：</h1><p>服务器和客户端的交互仅限于请求/响应过程，结束之后便断开，在下一次请求时，服务器会认为新的客户端。</p>
<p>为了维护他们之间的链接，让服务器知道这是前一个用户发送的请求，必须在一个地方保存客户端的信息。</p>
<p><strong>Cookie：</strong>通过在 客户端 记录的信息确定用户的身份。</p>
<p><strong>Session：</strong>通过在 服务器端 记录的信息确定用户的身份。</p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/01/20180501/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mr.Q">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dynamic-Soul">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/01/20180501/" itemprop="url">通用爬虫和聚焦爬虫</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-01T21:20:02+08:00">2018-05-01</time>
            

            
            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/爬虫/" itemprop="url" rel="index"><span itemprop="name">爬虫</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <hr>
<p>根据使用场景，网络爬虫可分为 <strong>通用爬虫</strong> 和<strong>聚焦爬虫</strong>两种</p>
<hr>
<h2 id="通用爬虫"><a href="#通用爬虫" class="headerlink" title="通用爬虫"></a>通用爬虫</h2><p>通用网络爬虫 是 捜索引擎抓取系统（Baidu、Google、Yahoo等）的重要组成部分。主要目的是将互联网上的网页下载到本地，形成一个互联网内容的镜像备份。</p>
<h3 id="1-通用搜索引擎（Search-Engine）工作原理"><a href="#1-通用搜索引擎（Search-Engine）工作原理" class="headerlink" title="1. 通用搜索引擎（Search Engine）工作原理"></a>1. 通用搜索引擎（Search Engine）工作原理</h3><p><strong>通用网络爬虫</strong> 从互联网中搜集网页，采集信息，这些网页信息用于为搜索引擎建立索引从而提供支持，它决定着整个引擎系统的内容是否丰富，信息是否即时，因此其性能的优劣直接影响着搜索引擎的效果。</p>
<h4 id="第一步：抓取网页"><a href="#第一步：抓取网页" class="headerlink" title="第一步：抓取网页"></a>第一步：抓取网页</h4><p>搜索引擎网络爬虫的基本工作流程如下：</p>
<pre><code>1.首先选取一部分的种子URL，将这些URL放入待抓取URL队列；

2.取出待抓取URL，解析DNS得到主机的IP，并将URL对应的网页下载下来，存储进已下载网页库中，并且将这些URL放进已抓取URL队列。

3.分析已抓取URL队列中的URL，分析其中的其他URL，并且将URL放入待抓取URL队列，从而进入下一个循环....
</code></pre><font color="gray">搜索引擎如何获取一个新网站的URL：</font><br><font color="gray">1. 新网站向搜索引擎主动提交网址：（如百度<a href="http://zhanzhang.baidu.com/linksubmit/url）" target="_blank" rel="noopener">http://zhanzhang.baidu.com/linksubmit/url）</a></font><br><font color="gray">2. 在其他网站上设置新网站外链（尽可能处于搜索引擎爬虫爬取范围）</font><br><font color="gray">3. 搜索引擎和DNS解析服务商(如DNSPod等）合作，新网站域名将被迅速抓取。</font>

<font color="gray">但是搜索引擎蜘蛛的爬行是被输入了一定的规则的，它需要遵从一些命令或文件的内容，如标注为nofollow的链接，或者是Robots协议。</font>

<pre><code>Robots协议（也叫爬虫协议、机器人协议等），全称是“网络爬虫排除标准”（Robots Exclusion Protocol），网站通过Robots协议告诉
搜索引擎哪些页面可以抓取，哪些页面不能抓取，例如：

淘宝网：https://www.taobao.com/robots.txt

腾讯网： http://www.qq.com/robots.txt
</code></pre><h4 id="第二步：数据存储"><a href="#第二步：数据存储" class="headerlink" title="第二步：数据存储"></a>第二步：数据存储</h4><p>搜索引擎通过爬虫爬取到的网页，将数据存入原始页面数据库。其中的页面数据与用户浏览器得到的HTML是完全一样的。</p>
<p>搜索引擎蜘蛛在抓取页面时，也做一定的重复内容检测，一旦遇到访问权重很低的网站上有大量抄袭、采集或者复制的内容，很可能就不再爬行。</p>
<h4 id="第三步：预处理"><a href="#第三步：预处理" class="headerlink" title="第三步：预处理"></a>第三步：预处理</h4><p>搜索引擎将爬虫抓取回来的页面，进行各种步骤的预处理。</p>
<ul>
<li>提取文字</li>
<li>中文分词</li>
<li>消除噪音（比如版权声明文字、导航条、广告等……）</li>
<li>索引处理</li>
<li>链接关系计算</li>
<li>特殊文件处理</li>
<li>….<br>除了HTML文件外，搜索引擎通常还能抓取和索引以文字为基础的多种文件类型，如 PDF、Word、WPS、XLS、PPT、TXT 文件等。我们在搜索结果中也经常会看到这些文件类型。</li>
</ul>
<p>但搜索引擎还不能处理图片、视频、Flash 这类非文字内容，也不能执行脚本和程序。</p>
<h4 id="第四步：提供检索服务，网站排名"><a href="#第四步：提供检索服务，网站排名" class="headerlink" title="第四步：提供检索服务，网站排名"></a>第四步：提供检索服务，网站排名</h4><p>搜索引擎在对信息进行组织和处理后，为用户提供关键字检索服务，将用户检索相关的信息展示给用户。</p>
<p>同时会根据页面的PageRank值（链接的访问量排名）来进行网站排名，这样Rank值高的网站在搜索结果中会排名较前，当然也可以直接使用 Money 购买搜索引擎网站排名，简单粗暴。</p>
<h3 id="但是，这些通用性搜索引擎也存在着一定的局限性："><a href="#但是，这些通用性搜索引擎也存在着一定的局限性：" class="headerlink" title="但是，这些通用性搜索引擎也存在着一定的局限性："></a>但是，这些通用性搜索引擎也存在着一定的局限性：</h3><pre><code>1.通用搜索引擎所返回的结果都是网页，而大多情况下，网页里90%的内容对用户来说都是无用的。

2.不同领域、不同背景的用户往往具有不同的检索目的和需求，搜索引擎无法提供针对具体某个用户的搜索结果。

3.万维网数据形式的丰富和网络技术的不断发展，图片、数据库、音频、视频多媒体等不同数据大量出现，通用搜索引擎对这些文件无能为力，不能很好地发现和获取。

4.通用搜索引擎大多提供基于关键字的检索，难以支持根据语义信息提出的查询，无法准确理解用户的具体需求。
</code></pre><h3 id="针对这些情况，聚焦爬虫技术得以广泛使用。"><a href="#针对这些情况，聚焦爬虫技术得以广泛使用。" class="headerlink" title="针对这些情况，聚焦爬虫技术得以广泛使用。"></a>针对这些情况，聚焦爬虫技术得以广泛使用。</h3><p>聚焦爬虫，是”面向特定主题需求”的一种网络爬虫程序，它与通用搜索引擎爬虫的区别在于： <em>聚焦爬虫在实施网页抓取时会对内容进行处理筛选，尽量保证只抓取与需求相关的网页信息。</em></p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/01/2018052922/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mr.Q">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dynamic-Soul">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/01/2018052922/" itemprop="url">图片下载器爬虫</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-01T17:53:31+08:00">2018-05-01</time>
            

            
            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/爬虫/" itemprop="url" rel="index"><span itemprop="name">爬虫</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="items-py"><a href="#items-py" class="headerlink" title="items.py"></a>items.py</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CoserItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    url = scrapy.Field()</span><br><span class="line">    name = scrapy.Field()</span><br><span class="line">    info = scrapy.Field()</span><br><span class="line">    image_urls = scrapy.Field()</span><br><span class="line">    images = scrapy.Field()</span><br></pre></td></tr></table></figure>
<h2 id="spiders-coser-py"><a href="#spiders-coser-py" class="headerlink" title="spiders/coser.py"></a>spiders/coser.py</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> scrapy.selector <span class="keyword">import</span> Selector</span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.contrib.loader <span class="keyword">import</span> ItemLoader</span><br><span class="line"><span class="keyword">from</span> Cosplay.items <span class="keyword">import</span> CoserItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CoserSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"coser"</span></span><br><span class="line">    allowed_domains = [<span class="string">"bcy.net"</span>]</span><br><span class="line">    start_urls = (</span><br><span class="line">        <span class="string">'http://bcy.net/cn125101'</span>,</span><br><span class="line">        <span class="string">'http://bcy.net/cn126487'</span>,</span><br><span class="line">        <span class="string">'http://bcy.net/cn126173'</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        sel = Selector(response)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> link <span class="keyword">in</span> sel.xpath(<span class="string">"//ul[@class='js-articles l-works']/li[@class='l-work--big']/article[@class='work work--second-created']/h2[@class='work__title']/a/@href"</span>).extract():</span><br><span class="line">            link = <span class="string">'http://bcy.net%s'</span> % link</span><br><span class="line">            request = scrapy.Request(link, callback=self.parse_item)</span><br><span class="line">            <span class="keyword">yield</span> request</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_item</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        l = ItemLoader(item=CoserItem(), response=response)</span><br><span class="line">        l.add_xpath(<span class="string">'name'</span>, <span class="string">"//h1[@class='js-post-title']/text()"</span>)</span><br><span class="line">        l.add_xpath(<span class="string">'info'</span>, <span class="string">"//div[@class='post__info']/div[@class='post__type post__info-group']/span/text()"</span>)</span><br><span class="line">        urls = l.get_xpath(<span class="string">'//img[@class="detail_std detail_clickable"]/@src'</span>)</span><br><span class="line">        urls = [url.replace(<span class="string">'/w650'</span>, <span class="string">''</span>) <span class="keyword">for</span> url <span class="keyword">in</span> urls]</span><br><span class="line">        l.add_value(<span class="string">'image_urls'</span>, urls)</span><br><span class="line">        l.add_value(<span class="string">'url'</span>, response.url)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> l.load_item()</span><br></pre></td></tr></table></figure>
<h2 id="pipelines-py"><a href="#pipelines-py" class="headerlink" title="pipelines.py"></a>pipelines.py</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> Cosplay <span class="keyword">import</span> settings</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ImageDownloadPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">'image_urls'</span> <span class="keyword">in</span> item:</span><br><span class="line">            images = []</span><br><span class="line">            dir_path = <span class="string">'%s/%s'</span> % (settings.IMAGES_STORE, spider.name)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(dir_path):</span><br><span class="line">                os.makedirs(dir_path)</span><br><span class="line">            <span class="keyword">for</span> image_url <span class="keyword">in</span> item[<span class="string">'image_urls'</span>]:</span><br><span class="line">                us = image_url.split(<span class="string">'/'</span>)[<span class="number">3</span>:]</span><br><span class="line">                image_file_name = <span class="string">'_'</span>.join(us)</span><br><span class="line">                file_path = <span class="string">'%s/%s'</span> % (dir_path, image_file_name)</span><br><span class="line">                images.append(file_path)</span><br><span class="line">                <span class="keyword">if</span> os.path.exists(file_path):</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">with</span> open(file_path, <span class="string">'wb'</span>) <span class="keyword">as</span> handle:</span><br><span class="line">                    response = requests.get(image_url, stream=<span class="keyword">True</span>)</span><br><span class="line">                    <span class="keyword">for</span> block <span class="keyword">in</span> response.iter_content(<span class="number">1024</span>):</span><br><span class="line">                        <span class="keyword">if</span> <span class="keyword">not</span> block:</span><br><span class="line">                            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">                        handle.write(block)</span><br><span class="line"></span><br><span class="line">            item[<span class="string">'images'</span>] = images</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>
<h2 id="settings-py"><a href="#settings-py" class="headerlink" title="settings.py"></a>settings.py</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;<span class="string">'Cosplay.pipelines.ImageDownloadPipeline'</span>: <span class="number">1</span>&#125;</span><br><span class="line"></span><br><span class="line">IMAGES_STORE = <span class="string">'../Images'</span></span><br><span class="line"></span><br><span class="line">DOWNLOAD_DELAY = <span class="number">0.25</span>    <span class="comment"># 250 ms of delay</span></span><br></pre></td></tr></table></figure>
<h2 id="在项目根目录下新建main-py文件-用于调试"><a href="#在项目根目录下新建main-py文件-用于调试" class="headerlink" title="在项目根目录下新建main.py文件,用于调试"></a>在项目根目录下新建main.py文件,用于调试</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> cmdline</span><br><span class="line">cmdline.execute(<span class="string">'scrapy crawl coser'</span>.split())</span><br></pre></td></tr></table></figure>
<h2 id="执行程序"><a href="#执行程序" class="headerlink" title="执行程序"></a>执行程序</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">py2 main.py</span><br></pre></td></tr></table></figure>
          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/03/18/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mr.Q">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dynamic-Soul">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/03/18/hello-world/" itemprop="url">爬虫基本内容</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-03-18T18:44:12+08:00">2018-03-18</time>
            

            
            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/爬虫/" itemprop="url" rel="index"><span itemprop="name">爬虫</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="1-什么是爬虫？"><a href="#1-什么是爬虫？" class="headerlink" title="1 . 什么是爬虫？"></a>1 . 什么是爬虫？</h3><p>爬虫：就是抓取网页数据的程序</p>
<h3 id="2-爬虫怎么抓取网页数据？"><a href="#2-爬虫怎么抓取网页数据？" class="headerlink" title="2 . 爬虫怎么抓取网页数据？"></a>2 . 爬虫怎么抓取网页数据？</h3><p>首先来看网页的三个特征：   </p>
<font color="#A52A2A" face="黑体">①每个网页都有自己唯一的的URL（统一资源定位符）来进行定位；<br>②网页使用HTML（超文本标记语言）来描述页面信息；<br>③网页使用HTTP/HTTPS（超文本传输协议）来传输HTML数据。</font>  

<p>爬虫的设计思路：  </p>
<p><font color="#A52A2A" face="黑体">①首先确定需要爬取网页的URL地址；<br>②通过HTTP/HTTPS协议来获取相应的HTML页面；<br>③提取HTML页面里有用的数据<br>&nbsp;&nbsp;a.如果是需要的数据，保存；<br>&nbsp;&nbsp;b.如果是页面里的其他URL，那就继续执行第②步。</font>  </p>
<h3 id="3-为什么选择Python语言写爬虫？"><a href="#3-为什么选择Python语言写爬虫？" class="headerlink" title="3.为什么选择Python语言写爬虫？"></a>3.为什么选择Python语言写爬虫？</h3><p>&nbsp;&nbsp;可以写爬虫的语言很多，如PHP、Java、C/C++、Python等等…  </p>
<p>&nbsp;&nbsp;①PHP虽然是世界上最好的语言（滑稽），但是它天生不是干这个的，而且对多线程、异步支持不够好，并发处理能力很弱。<br>&nbsp;&nbsp;爬虫是工具性程序，对速度和效率要求比较高。  </p>
<p>&nbsp;&nbsp;②Java的爬虫生态圈也很晚上，是Python爬虫最大的对手。但是Java语言本身笨重，代码量大。重构成本较高，任何修改都会导致代码的大量改动。<br>&nbsp;&nbsp;爬虫经常需要修改部分采集代码。  </p>
<p>&nbsp;&nbsp;③C/C++运行效率和性能几乎最强，但是学习成本很高。代码成型比较慢。<br>&nbsp;&nbsp;能用C/C++写爬虫，是能力的表现，但不是最好的选择。  </p>
<p>&nbsp;&nbsp;④Python语法优美，代码简洁，开发效率高，支持的模块多，相关的HTTP请求模块和HTML解析模块非常丰富。<br>&nbsp;&nbsp;还有强大的爬虫Scrapy，以及成熟高效的Scrapy—redis分布式策略。而且，调用其他接口也非常方便（胶水语言）。</p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/4/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.gif"
                alt="Mr.Q" />
            
              <p class="site-author-name" itemprop="name">Mr.Q</p>
              <p class="site-description motion-element" itemprop="description">我希望时间没有流走，世界还停在过去，昏黄的光还照在旧日的路上，我刚刚走过，恰与你们相识。</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">50</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                    <span class="site-state-item-count">1</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">7</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          
          

          
          

          
            
          
          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Mr.Q</span>

  

  
</div>




  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/theme-next/hexo-theme-next">NexT.Gemini</a> v6.0.6</div>




<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共61.1k字</span>
</div>
        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>














  













  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.0.6"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.0.6"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=6.0.6"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=6.0.6"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.0.6"></script>



  



	





  





  










  





  

  

  

  

  
  

  

  

  

  


  
</body>
</html>
