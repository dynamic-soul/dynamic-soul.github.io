<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-CN">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222">






  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.0.6" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.0.6">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.0.6">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.0.6">


  <link rel="mask-icon" href="/images/logo.svg?v=6.0.6" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '6.0.6',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="我希望时间没有流走，世界还停在过去，昏黄的光还照在旧日的路上，我刚刚走过，恰与你们相识。">
<meta property="og:type" content="website">
<meta property="og:title" content="Dynamic-Soul">
<meta property="og:url" content="http://yoursite.com/page/8/index.html">
<meta property="og:site_name" content="Dynamic-Soul">
<meta property="og:description" content="我希望时间没有流走，世界还停在过去，昏黄的光还照在旧日的路上，我刚刚走过，恰与你们相识。">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Dynamic-Soul">
<meta name="twitter:description" content="我希望时间没有流走，世界还停在过去，昏黄的光还照在旧日的路上，我刚刚走过，恰与你们相识。">






  <link rel="canonical" href="http://yoursite.com/page/8/"/>



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Dynamic-Soul</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <a href="https://github.com/dynamic-soul"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_gray_6d6d6d.png" alt="Fork me on GitHub"></a>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"> 

<div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Dynamic-Soul</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        
          
  <li class="menu-item menu-item-home">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br />首页</a>
</li>

      
        
        
          
  <li class="menu-item menu-item-about">
    <a href="/about/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-user"></i> <br />关于</a>
</li>

      
        
        
          
  <li class="menu-item menu-item-tags">
    <a href="/tags/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />标签</a>
</li>

      
        
        
          
  <li class="menu-item menu-item-categories">
    <a href="/categories/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-th"></i> <br />分类</a>
</li>

      
        
        
          
  <li class="menu-item menu-item-archives">
    <a href="/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />归档</a>
</li>

      

      
    </ul>
  

  

  
</nav>


  



 </div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/03/2018050321/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mr.Q">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dynamic-Soul">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/03/2018050321/" itemprop="url">urllib2：GET请求和POST请求</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-03T21:27:30+08:00">2018-05-03</time>
            

            
            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/爬虫/" itemprop="url" rel="index"><span itemprop="name">爬虫</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="urllib2默认只支持HTTP-HTTPS的GET和POST方法"><a href="#urllib2默认只支持HTTP-HTTPS的GET和POST方法" class="headerlink" title="urllib2默认只支持HTTP/HTTPS的GET和POST方法"></a>urllib2默认只支持HTTP/HTTPS的GET和POST方法</h1><h2 id="urllib-urlencode"><a href="#urllib-urlencode" class="headerlink" title="urllib.urlencode()"></a>urllib.urlencode()</h2><p><strong>urllib 和 urllib2 都是接受URL请求的相关模块，但是提供了不同的功能。两个最显著的不同如下：</strong></p>
<blockquote>
<p><em>urllib 仅可以接受URL，不能创建 设置了headers 的Request 类实例；
</em>但是 urllib 提供 urlencode 方法用来GET查询字符串的产生，而 urllib2 则没有。（这是 urllib 和 urllib2 经常一起使用的主要原因）<br>*编码工作使用urllib的urlencode()函数，帮我们将key:value这样的键值对转换成”key=value”这样的字符串，解码工作可以使用urllib的unquote()函数。（注意，不是urllib2.urlencode() )<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># IPython2 中的测试结果</span></span><br><span class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> urllib</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: word = &#123;<span class="string">"wd"</span> : <span class="string">"炉石传说"</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过urllib.urlencode()方法，将字典键值对按URL编码转换，从而能被web服务器接受。</span></span><br><span class="line">In [<span class="number">3</span>]: urllib.urlencode(word)  </span><br><span class="line">Out[<span class="number">3</span>]: <span class="string">"wd=%e7%82%89%e7%9f%b3%e4%bc%a0%e8%af%b4"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过urllib.unquote()方法，把 URL编码字符串，转换回原先字符串。</span></span><br><span class="line">In [<span class="number">4</span>]: <span class="keyword">print</span> urllib.unquote(<span class="string">"wd=%e7%82%89%e7%9f%b3%e4%bc%a0%e8%af%b4"</span>)</span><br><span class="line">wd=炉石传说</span><br></pre></td></tr></table></figure></p>
</blockquote>
<p><strong>一般HTTP请求提交数据，需要编码成 URL编码格式，然后做为url的一部分，或者作为参数传到Request对象中。</strong></p>
<h1 id="Get方式"><a href="#Get方式" class="headerlink" title="Get方式"></a>Get方式</h1><p>GET请求一般用于向服务器获取数据，比如说，用百度搜索炉石传说：<a href="https://www.baidu.com/s?ie=UTF-8&amp;wd=炉石传说" target="_blank" rel="noopener">https://www.baidu.com/s?ie=UTF-8&amp;wd=炉石传说</a><br>浏览器的url会跳转成:<a href="https://www.baidu.com/s?ie=UTF-8&amp;wd=%E7%82%89%E7%9F%B3%E4%BC%A0%E8%AF%B4" target="_blank" rel="noopener">https://www.baidu.com/s?ie=UTF-8&amp;wd=%E7%82%89%E7%9F%B3%E4%BC%A0%E8%AF%B4</a><br>在其中可以看到在请求部分里，<a href="http://www.baidu.com/s" target="_blank" rel="noopener">http://www.baidu.com/s</a>? 之后出现一个长长的字符串，其中就包含要查询的关键词炉石传说，于是可以尝试用默认的Get方式来发送请求。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># urllib2_get.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> urllib      <span class="comment">#负责url编码处理</span></span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"></span><br><span class="line">url = <span class="string">"http://www.baidu.com/s"</span></span><br><span class="line">word = &#123;<span class="string">"wd"</span>:<span class="string">"炉石传说"</span>&#125;</span><br><span class="line">word = urllib.urlencode(word) <span class="comment">#转换成url编码格式（字符串）</span></span><br><span class="line">newurl = url + <span class="string">"?"</span> + word    <span class="comment"># url首个分隔符就是 ?</span></span><br><span class="line"></span><br><span class="line">headers=&#123; <span class="string">"User-Agent"</span>: <span class="string">"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36"</span>&#125;</span><br><span class="line"></span><br><span class="line">request = urllib2.Request(newurl, headers=headers)</span><br><span class="line"></span><br><span class="line">response = urllib2.urlopen(request)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> response.read()</span><br></pre></td></tr></table></figure></p>
<h1 id="批量爬取贴吧页面数据"><a href="#批量爬取贴吧页面数据" class="headerlink" title="批量爬取贴吧页面数据"></a>批量爬取贴吧页面数据</h1><p>首先创建一个python文件, tiebaSpider.py，要完成的是，输入一个百度贴吧的地址，比如：</p>
<p>百度贴吧LOL吧第一页：<a href="http://tieba.baidu.com/f?kw=lol&amp;ie=utf-8&amp;pn=0" target="_blank" rel="noopener">http://tieba.baidu.com/f?kw=lol&amp;ie=utf-8&amp;pn=0</a></p>
<p>第二页： <a href="http://tieba.baidu.com/f?kw=lol&amp;ie=utf-8&amp;pn=50" target="_blank" rel="noopener">http://tieba.baidu.com/f?kw=lol&amp;ie=utf-8&amp;pn=50</a></p>
<p>第三页： <a href="http://tieba.baidu.com/f?kw=lol&amp;ie=utf-8&amp;pn=100" target="_blank" rel="noopener">http://tieba.baidu.com/f?kw=lol&amp;ie=utf-8&amp;pn=100</a></p>
<p>发现规律了吧，贴吧中每个页面不同之处，就是url最后的pn的值，其余的都是一样的，就可以抓住这个规律。<br>简单写一个小爬虫程序，来爬取百度LOL吧的所有网页。</p>
<ul>
<li>先写一个main，提示用户输入要爬取的贴吧名，并用urllib.urlencode()进行转码，然后组合url，假设是lol吧，那么组合后的url就是：<a href="http://tieba.baidu.com/f?kw=lol" target="_blank" rel="noopener">http://tieba.baidu.com/f?kw=lol</a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模拟 main 函数</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line"></span><br><span class="line">    kw = raw_input(<span class="string">"请输入需要爬取的贴吧:"</span>)</span><br><span class="line">    <span class="comment"># 输入起始页和终止页，str转成int类型</span></span><br><span class="line">    beginPage = int(raw_input(<span class="string">"请输入起始页："</span>))</span><br><span class="line">    endPage = int(raw_input(<span class="string">"请输入终止页："</span>))</span><br><span class="line"></span><br><span class="line">    url = <span class="string">"http://tieba.baidu.com/f?"</span></span><br><span class="line">    key = urllib.urlencode(&#123;<span class="string">"kw"</span> : kw&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 组合后的url示例：http://tieba.baidu.com/f?kw=lol</span></span><br><span class="line">    url = url + key</span><br><span class="line">    tiebaSpider(url, beginPage, endPage)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li><p>接下来，写一个百度贴吧爬虫接口，需要传递3个参数给这个接口， 一个是main里组合的url地址，以及起始页码和终止页码，表示要爬取页码的范围。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tiebaSpider</span><span class="params">(url, beginPage, endPage)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">        作用：负责处理url，分配每个url去发送请求</span></span><br><span class="line"><span class="string">        url：需要处理的第一个url</span></span><br><span class="line"><span class="string">        beginPage: 爬虫执行的起始页面</span></span><br><span class="line"><span class="string">        endPage: 爬虫执行的截止页面</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> range(beginPage, endPage + <span class="number">1</span>):</span><br><span class="line">        pn = (page - <span class="number">1</span>) * <span class="number">50</span></span><br><span class="line"></span><br><span class="line">        filename = <span class="string">"第"</span> + str(page) + <span class="string">"页.html"</span></span><br><span class="line">        <span class="comment"># 组合为完整的 url，并且pn值每次增加50</span></span><br><span class="line">        fullurl = url + <span class="string">"&amp;pn="</span> + str(pn)</span><br><span class="line">        <span class="comment">#print fullurl</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 调用loadPage()发送请求获取HTML页面</span></span><br><span class="line">        html = loadPage(fullurl, filename)</span><br><span class="line">        <span class="comment"># 将获取到的HTML页面写入本地磁盘文件</span></span><br><span class="line">        writeFile(html, filename)</span><br></pre></td></tr></table></figure>
</li>
<li><p>之前已经写出一个爬取一个网页的代码。现在，我们可以将它封装成一个小函数loadPage，供我们使用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadPage</span><span class="params">(url, filename)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">        作用：根据url发送请求，获取服务器响应文件</span></span><br><span class="line"><span class="string">        url：需要爬取的url地址</span></span><br><span class="line"><span class="string">        filename: 文件名</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">"正在下载"</span> + filename</span><br><span class="line"></span><br><span class="line">    headers = &#123;<span class="string">"User-Agent"</span>: <span class="string">"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0;"</span>&#125;</span><br><span class="line"></span><br><span class="line">    request = urllib2.Request(url, headers = headers)</span><br><span class="line">    response = urllib2.urlopen(request)</span><br><span class="line">    <span class="keyword">return</span> response.read()</span><br></pre></td></tr></table></figure>
</li>
<li><p>最后如果希望将爬取到了每页的信息存储在本地磁盘上，我们可以简单写一个存储文件的接口。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">writeFile</span><span class="params">(html, filename)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">        作用：保存服务器响应文件到本地磁盘文件里</span></span><br><span class="line"><span class="string">        html: 服务器响应文件</span></span><br><span class="line"><span class="string">        filename: 本地磁盘文件名</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">"正在存储"</span> + filename</span><br><span class="line">    <span class="keyword">with</span> open(filename, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(html)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"-"</span> * <span class="number">20</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>其实很多网站都是这样的，同类网站下的html页面编号，分别对应网址后的网页序号，只要发现规律就可以批量爬取页面了。</p>
<h1 id="POST方式："><a href="#POST方式：" class="headerlink" title="POST方式："></a>POST方式：</h1><p>上面我们说了Request请求对象的里有data参数，它就是用在POST里的，我们要传送的数据就是这个参数data，data是一个字典，里面要匹配键值对。</p>
<h2 id="有道词典翻译网站："><a href="#有道词典翻译网站：" class="headerlink" title="有道词典翻译网站："></a>有道词典翻译网站：</h2><p>输入测试数据，再通过使用Fiddler观察，其中有一条是POST请求，而向服务器发送的请求数据并不是在url里，那么我们可以试着模拟这个POST请求。<br><img src="/2018/05/03/2018050321/youdaopost.jpg"><br>于是，我们可以尝试用POST方式发送请求。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"></span><br><span class="line"><span class="comment"># POST请求的目标URL</span></span><br><span class="line">url = <span class="string">"http://fanyi.youdao.com/translate?smartresult=dict&amp;smartresult=rule&amp;smartresult=ugc&amp;sessionFrom=null"</span></span><br><span class="line"></span><br><span class="line">headers=&#123;<span class="string">"User-Agent"</span>: <span class="string">"Mozilla...."</span>&#125;</span><br><span class="line"></span><br><span class="line">formdata = &#123;</span><br><span class="line">    <span class="string">"type"</span>:<span class="string">"AUTO"</span>,</span><br><span class="line">    <span class="string">"i"</span>:<span class="string">"i love python"</span>,</span><br><span class="line">    <span class="string">"doctype"</span>:<span class="string">"json"</span>,</span><br><span class="line">    <span class="string">"xmlVersion"</span>:<span class="string">"1.8"</span>,</span><br><span class="line">    <span class="string">"keyfrom"</span>:<span class="string">"fanyi.web"</span>,</span><br><span class="line">    <span class="string">"ue"</span>:<span class="string">"UTF-8"</span>,</span><br><span class="line">    <span class="string">"action"</span>:<span class="string">"FY_BY_ENTER"</span>,</span><br><span class="line">    <span class="string">"typoResult"</span>:<span class="string">"true"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">data = urllib.urlencode(formdata)</span><br><span class="line"></span><br><span class="line">request = urllib2.Request(url, data = data, headers = headers)</span><br><span class="line">response = urllib2.urlopen(request)</span><br><span class="line"><span class="keyword">print</span> response.read()</span><br></pre></td></tr></table></figure></p>
<p><strong>发送POST请求时，需要特别注意headers的一些属性：</strong></p>
<pre><code>Content-Length: 144： 是指发送的表单数据长度为144，也就是字符个数是144个。

X-Requested-With: XMLHttpRequest ：表示Ajax异步请求。

Content-Type: application/x-www-form-urlencoded ： 表示浏览器提交 Web 表单时使用，表单数据会按照 name1=value1&amp;name2=value2 键值对形式进行编码。
</code></pre><h1 id="获取AJAX加载的内容"><a href="#获取AJAX加载的内容" class="headerlink" title="获取AJAX加载的内容"></a>获取AJAX加载的内容</h1><p>有些网页内容使用AJAX加载，只要记得，AJAX一般返回的是JSON,直接对AJAX地址进行post或get，就返回JSON数据了。</p>
<p>“作为一名爬虫工程师，你最需要关注的，是数据的来源”<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"></span><br><span class="line"><span class="comment"># demo1</span></span><br><span class="line"></span><br><span class="line">url = <span class="string">"https://movie.douban.com/j/chart/top_list?type=11&amp;interval_id=100%3A90&amp;action"</span></span><br><span class="line"></span><br><span class="line">headers=&#123;<span class="string">"User-Agent"</span>: <span class="string">"Mozilla...."</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 变动的是这两个参数，从start开始往后显示limit个</span></span><br><span class="line">formdata = &#123;</span><br><span class="line">    <span class="string">'start'</span>:<span class="string">'0'</span>,</span><br><span class="line">    <span class="string">'limit'</span>:<span class="string">'10'</span></span><br><span class="line">&#125;</span><br><span class="line">data = urllib.urlencode(formdata)</span><br><span class="line"></span><br><span class="line">request = urllib2.Request(url, data = data, headers = headers)</span><br><span class="line">response = urllib2.urlopen(request)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> response.read()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># demo2</span></span><br><span class="line"></span><br><span class="line">url = <span class="string">"https://movie.douban.com/j/chart/top_list?"</span></span><br><span class="line">headers=&#123;<span class="string">"User-Agent"</span>: <span class="string">"Mozilla...."</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理所有参数</span></span><br><span class="line">formdata = &#123;</span><br><span class="line">    <span class="string">'type'</span>:<span class="string">'11'</span>,</span><br><span class="line">    <span class="string">'interval_id'</span>:<span class="string">'100:90'</span>,</span><br><span class="line">    <span class="string">'action'</span>:<span class="string">''</span>,</span><br><span class="line">    <span class="string">'start'</span>:<span class="string">'0'</span>,</span><br><span class="line">    <span class="string">'limit'</span>:<span class="string">'10'</span></span><br><span class="line">&#125;</span><br><span class="line">data = urllib.urlencode(formdata)</span><br><span class="line"></span><br><span class="line">request = urllib2.Request(url, data = data, headers = headers)</span><br><span class="line">response = urllib2.urlopen(request)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> response.read()</span><br></pre></td></tr></table></figure></p>
<p><strong>问题：为什么有时候POST也能在URL内看到数据？</strong></p>
<blockquote>
<ul>
<li>GET方式是直接以链接形式访问，链接中包含了所有的参数，服务器端用Request.QueryString获取变量的值。如果包含了密码的话是一种不安全的选择，不过你可以直观地看到自己提交了什么内容。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>POST则不会在网址上显示所有的参数，服务器端用Request.Form获取提交的数据，在Form提交的时候。但是HTML代码里如果不指定 method 属性，则默认为GET请求，Form中提交的数据将会附加在url之后，以?分开与url分开。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>表单数据可以作为 URL 字段（method=”get”）或者 HTTP POST （method=”post”）的方式来发送。比如在下面的HTML代码中，表单数据将因为 （method=”get”） 而附加到 URL 上：<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;form action=<span class="string">"form_action.asp"</span> method=<span class="string">"get"</span>&gt;</span><br><span class="line">    &lt;p&gt;First name: &lt;input type="text" name="fname" /&gt;&lt;/p&gt;</span><br><span class="line">    &lt;p&gt;Last name: &lt;input type="text" name="lname" /&gt;&lt;/p&gt;</span><br><span class="line">    &lt;input type=<span class="string">"submit"</span> value=<span class="string">"Submit"</span> /&gt;</span><br><span class="line">&lt;/form&gt;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</blockquote>
<h2 id="处理HTTPS请求-SSL证书验证"><a href="#处理HTTPS请求-SSL证书验证" class="headerlink" title="处理HTTPS请求 SSL证书验证"></a>处理HTTPS请求 SSL证书验证</h2><p>现在随处可见 https 开头的网站，urllib2可以为 HTTPS 请求验证SSL证书，就像web浏览器一样，如果网站的SSL证书是经过CA认证的，则能够正常访问，如：<a href="https://www.baidu.com/等.." target="_blank" rel="noopener">https://www.baidu.com/等..</a>.</p>
<p>如果SSL证书验证不通过，或者操作系统不信任服务器的安全证书，比如浏览器在访问12306网站如：<a href="https://www.12306.cn/mormhweb/的时候，会警告用户证书不受信任。（据说" target="_blank" rel="noopener">https://www.12306.cn/mormhweb/的时候，会警告用户证书不受信任。（据说</a> 12306 网站证书是自己做的，没有通过CA认证）<br>urllib2在访问的时候则会报出SSLError：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"></span><br><span class="line">url = <span class="string">"https://www.12306.cn/mormhweb/"</span></span><br><span class="line"></span><br><span class="line">headers = &#123;<span class="string">"User-Agent"</span>: <span class="string">"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36"</span>&#125;</span><br><span class="line"></span><br><span class="line">request = urllib2.Request(url, headers = headers)</span><br><span class="line"></span><br><span class="line">response = urllib2.urlopen(request)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> response.read()</span><br></pre></td></tr></table></figure></p>
<p>运行结果：<br>urllib2.URLError: <urlopen error="" [ssl:="" certificate_verify_failed]="" certificate="" verify="" failed="" (_ssl.c:590)=""></urlopen></p>
<p>所以，如果以后遇到这种网站，需要单独处理SSL证书，让程序忽略SSL证书验证错误，即可正常访问。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"><span class="comment"># 1. 导入Python SSL处理模块</span></span><br><span class="line"><span class="keyword">import</span> ssl</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 表示忽略未经核实的SSL证书认证</span></span><br><span class="line">context = ssl._create_unverified_context()</span><br><span class="line"></span><br><span class="line">url = <span class="string">"https://www.12306.cn/mormhweb/"</span></span><br><span class="line"></span><br><span class="line">headers = &#123;<span class="string">"User-Agent"</span>: <span class="string">"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36"</span>&#125;</span><br><span class="line"></span><br><span class="line">request = urllib2.Request(url, headers = headers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 在urlopen()方法里 指明添加 context 参数</span></span><br><span class="line">response = urllib2.urlopen(request, context = context)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> response.read()</span><br></pre></td></tr></table></figure></p>
<h2 id="关于CA"><a href="#关于CA" class="headerlink" title="关于CA"></a>关于CA</h2><p>CA(Certificate Authority)是数字证书认证中心的简称，是指发放、管理、废除数字证书的受信任的第三方机构，如北京数字认证股份有限公司、上海市数字证书认证中心有限公司等…</p>
<p>CA的作用是检查证书持有者身份的合法性，并签发证书，以防证书被伪造或篡改，以及对证书和密钥进行管理。</p>
<p>现实生活中可以用身份证来证明身份， 那么在网络世界里，数字证书就是身份证。和现实生活不同的是，并不是每个上网的用户都有数字证书的，往往只有当一个人需要证明自己的身份的时候才需要用到数字证书。</p>
<p>普通用户一般是不需要，因为网站并不关心是谁访问了网站，现在的网站只关心流量。但是反过来，网站就需要证明自己的身份了。</p>
<p>比如说现在钓鱼网站很多的，比如你想访问的是<a href="http://www.baidu.com，但其实你访问的是www.daibu.com”，所以在提交自己的隐私信息之前需要验证一下网站的身份，要求网站出示数字证书。" target="_blank" rel="noopener">www.baidu.com，但其实你访问的是www.daibu.com”，所以在提交自己的隐私信息之前需要验证一下网站的身份，要求网站出示数字证书。</a></p>
<p>一般正常的网站都会主动出示自己的数字证书，来确保客户端和网站服务器之间的通信数据是加密安全的。</p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/03/2018050313/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mr.Q">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dynamic-Soul">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/03/2018050313/" itemprop="url">urllib2模块的基本使用</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-03T13:29:27+08:00">2018-05-03</time>
            

            
            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/爬虫/" itemprop="url" rel="index"><span itemprop="name">爬虫</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="urllib2库的基本使用"><a href="#urllib2库的基本使用" class="headerlink" title="urllib2库的基本使用"></a>urllib2库的基本使用</h1><hr>
<p>所谓网页抓取，就是把URL地址中指定的网络资源从网络流中读取出来，保存到本地。 在Python中有很多库可以用来抓取网页，先来了解urllib2。</p>
<pre><code>urllib2 是 Python2.7 自带的模块(不需要下载，导入即可使用)

urllib2 官方文档：https://docs.python.org/2/library/urllib2.html

urllib2 源码：https://hg.python.org/cpython/file/2.7/Lib/urllib2.py
</code></pre><p>urllib2 在 python3.x 中被改为urllib.request</p>
<h2 id="urlopen"><a href="#urlopen" class="headerlink" title="urlopen"></a>urlopen</h2><p>先来段代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># urllib2_urlopen.py</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入urllib2 库</span></span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 向指定的url发送请求，并返回服务器响应的类文件对象</span></span><br><span class="line">response = urllib2.urlopen(<span class="string">"http://www.baidu.com"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 类文件对象支持 文件对象的操作方法，如read()方法读取文件全部内容，返回字符串</span></span><br><span class="line">html = response.read()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印字符串</span></span><br><span class="line"><span class="keyword">print</span> html</span><br></pre></td></tr></table></figure></p>
<p>执行写的python代码，将打印结果<br>Power@PowerMac ~$: python urllib2_urlopen.py<br><strong>实际上，如果我们在浏览器上打开百度主页， 右键选择“查看源代码”，你会发现，跟我们刚才打印出来的是一模一样。也就是说，上面的4行代码就已经帮我们把百度的首页的全部代码爬了下来。<br>一个基本的url请求对应的python代码真的非常简单。</strong></p>
<h2 id="Request"><a href="#Request" class="headerlink" title="Request"></a>Request</h2><p>在第一个例子里，urlopen()的参数就是一个url地址；</p>
<p>但是如果需要执行更复杂的操作，比如增加HTTP报头，必须创建一个 Request 实例来作为urlopen()的参数；而需要访问的url地址则作为 Request 实例的参数。</p>
<p>我们编辑urllib2_request.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># urllib2_request.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"></span><br><span class="line"><span class="comment"># url 作为Request()方法的参数，构造并返回一个Request对象</span></span><br><span class="line">request = urllib2.Request(<span class="string">"http://www.baidu.com"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Request对象作为urlopen()方法的参数，发送给服务器并接收响应</span></span><br><span class="line">response = urllib2.urlopen(request)</span><br><span class="line"></span><br><span class="line">html = response.read()</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> html</span><br></pre></td></tr></table></figure>
<p><strong>运行结果是完全一样的：</strong></p>
<pre><code>新建Request实例，除了必须要有 url 参数之外，还可以设置另外两个参数：

   1. data（默认空）：是伴随 url 提交的数据（比如要post的数据），同时 HTTP 请求将从 &quot;GET&quot;方式 改为 &quot;POST&quot;方式。

   2. headers（默认空）：是一个字典，包含了需要发送的HTTP报头的键值对。

这两个参数下面会说到。
</code></pre><p>##　User-Agent<br>但是这样直接用urllib2给一个网站发送请求的话，确实略有些唐突了，就好比，人家每家都有门，你以一个路人的身份直接闯进去显然不是很礼貌。而且有一些站点不喜欢被程序（非人为访问）访问，有可能会拒绝你的访问请求。</p>
<p>但是如果我们用一个合法的身份去请求别人网站，显然人家就是欢迎的，所以我们就应该给我们的这个代码加上一个身份，就是所谓的User-Agent头。</p>
<ul>
<li>浏览器 就是互联网世界上公认被允许的身份，如果我们希望我们的爬虫程序更像一个真实用户，那我们第一步，就是需要伪装成一个被公认的浏览器。用不同的浏览器在发送请求的时候，会有不同的User-Agent头。 urllib2默认的User-Agent头为：Python-urllib/x.y（x和y是Python主版本和次版本号,例如 Python-urllib/2.7）<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#urllib2_useragent.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"></span><br><span class="line">url = <span class="string">"http://www.itcast.cn"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#IE 9.0 的 User-Agent，包含在 ua_header里</span></span><br><span class="line">ua_header = &#123;<span class="string">"User-Agent"</span> : <span class="string">"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0;"</span>&#125; </span><br><span class="line"></span><br><span class="line"><span class="comment">#  url 连同 headers，一起构造Request请求，这个请求将附带 IE9.0 浏览器的User-Agent</span></span><br><span class="line">request = urllib2.Request(url, headers = ua_header)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 向服务器发送这个请求</span></span><br><span class="line">response = urllib2.urlopen(request)</span><br><span class="line"></span><br><span class="line">html = response.read()</span><br><span class="line"><span class="keyword">print</span> html</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>##　添加更多的Header信息</p>
<p>在 HTTP Request 中加入特定的 Header，来构造一个完整的HTTP请求消息。</p>
<pre><code>可以通过调用Request.add_header() 添加/修改一个特定的header 也可以通过调用Request.get_header()来查看已有的header。
</code></pre><ul>
<li>添加一个特定的header</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># urllib2_headers.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"></span><br><span class="line">url = <span class="string">"http://www.itcast.cn"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#IE 9.0 的 User-Agent</span></span><br><span class="line">header = &#123;<span class="string">"User-Agent"</span> : <span class="string">"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0;"</span>&#125; </span><br><span class="line">request = urllib2.Request(url, headers = header)</span><br><span class="line"></span><br><span class="line"><span class="comment">#也可以通过调用Request.add_header() 添加/修改一个特定的header</span></span><br><span class="line">request.add_header(<span class="string">"Connection"</span>, <span class="string">"keep-alive"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 也可以通过调用Request.get_header()来查看header信息</span></span><br><span class="line"><span class="comment"># request.get_header(header_name="Connection")</span></span><br><span class="line"></span><br><span class="line">response = urllib2.urlopen(req)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> response.code     <span class="comment">#可以查看响应状态码</span></span><br><span class="line">html = response.read()</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> html</span><br></pre></td></tr></table></figure>
<ul>
<li>随机添加/修改User-Agent<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># urllib2_add_headers.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">url = <span class="string">"http://www.itcast.cn"</span></span><br><span class="line"></span><br><span class="line">ua_list = [</span><br><span class="line">    <span class="string">"Mozilla/5.0 (Windows NT 6.1; ) Apple.... "</span>,</span><br><span class="line">    <span class="string">"Mozilla/5.0 (X11; CrOS i686 2268.111.0)... "</span>,</span><br><span class="line">    <span class="string">"Mozilla/5.0 (Macintosh; U; PPC Mac OS X.... "</span>,</span><br><span class="line">    <span class="string">"Mozilla/5.0 (Macintosh; Intel Mac OS... "</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">user_agent = random.choice(ua_list)</span><br><span class="line"></span><br><span class="line">request = urllib2.Request(url)</span><br><span class="line"></span><br><span class="line"><span class="comment">#也可以通过调用Request.add_header() 添加/修改一个特定的header</span></span><br><span class="line">request.add_header(<span class="string">"User-Agent"</span>, user_agent)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一个字母大写，后面的全部小写</span></span><br><span class="line">request.get_header(<span class="string">"User-agent"</span>)</span><br><span class="line"></span><br><span class="line">response = urllib2.urlopen(req)</span><br><span class="line"></span><br><span class="line">html = response.read()</span><br><span class="line"><span class="keyword">print</span> html</span><br></pre></td></tr></table></figure></li>
</ul>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/02/2018050213/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mr.Q">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dynamic-Soul">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/02/2018050213/" itemprop="url">HTTP/HTTPS的请求与响应</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-02T13:23:30+08:00">2018-05-02</time>
            

            
            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/爬虫/" itemprop="url" rel="index"><span itemprop="name">爬虫</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <hr>
<h1 id="HTTP和HTTPS"><a href="#HTTP和HTTPS" class="headerlink" title="HTTP和HTTPS"></a>HTTP和HTTPS</h1><hr>
<p>HTTP协议（HyperText Transfer Protocol，超文本传输协议）：是一种发布和接收 HTML页面的方法。</p>
<p>HTTPS（Hypertext Transfer Protocol over Secure Socket Layer）简单讲是HTTP的安全版，在HTTP下加入SSL层。</p>
<p>SSL（Secure Sockets Layer 安全套接层）主要用于Web的安全传输协议，在传输层对网络连接进行加密，保障在Internet上数据传输的安全。</p>
<ul>
<li>HTTP的端口号为80，</li>
<li>HTTPS的端口号为443<h2 id="HTTP工作原理"><a href="#HTTP工作原理" class="headerlink" title="HTTP工作原理"></a>HTTP工作原理</h2>网络爬虫抓取过程可以理解为模拟浏览器操作的过程。<br>浏览器的主要功能是向服务器发出请求，在浏览器窗口中展示您选择的网络资源，HTTP是一套计算机通过网络进行通信的规则。</li>
</ul>
<h1 id="HTTP的请求与响应"><a href="#HTTP的请求与响应" class="headerlink" title="HTTP的请求与响应"></a>HTTP的请求与响应</h1><p>HTTP通信由两部分组成： <strong>客户端请求消息</strong> 与 <strong>服务器响应消息</strong></p>
<p>##浏览器发送HTTP请求的过程：</p>
<pre><code>1.当用户在浏览器的地址栏中输入一个URL并按回车键之后，浏览器会向HTTP服务器发送HTTP请求。HTTP请求主要分为“Get”和“Post”两种方法。

2.当我们在浏览器输入URL http://www.baidu.com 的时候，浏览器发送一个Request请求去获取 http://www.baidu.com 的html文件，服务器把Response文件对象发送回给浏览器。

3.浏览器分析Response中的 HTML，发现其中引用了很多其他文件，比如Images文件，CSS文件，JS文件。 浏览器会自动再次发送Request去获取图片，CSS文件，或者JS文件。

4.当所有的文件都下载成功后，网页会根据HTML语法结构，完整的显示出来了。
</code></pre><p>URL（Uniform / Universal Resource Locator的缩写）：统一资源定位符，是用于完整地描述Internet上网页和其他资源的地址的一种标识方法。</p>
<p>基本格式：scheme://host[:port#]/path/…/[?query-string][#anchor]</p>
<ul>
<li>scheme：协议(例如：http, https, ftp)</li>
<li>host：服务器的IP地址或者域名</li>
<li>port#：服务器的端口（如果是走协议默认端口，缺省端口80）</li>
<li>path：访问资源的路径</li>
<li>query-string：参数，发送给http服务器的数据</li>
<li>anchor：锚（跳转到网页的指定锚点位置）</li>
</ul>
<p>例如：</p>
<p><a href="http://www.baidu.com" target="_blank" rel="noopener">http://www.baidu.com</a></p>
<p><a href="http://item.jd.com/11936238.html#product-detail" target="_blank" rel="noopener">http://item.jd.com/11936238.html#product-detail</a></p>
<h1 id="客户端HTTP请求"><a href="#客户端HTTP请求" class="headerlink" title="客户端HTTP请求"></a>客户端HTTP请求</h1><p>URL只是标识资源的位置，而HTTP是用来提交和获取资源。客户端发送一个HTTP请求到服务器的请求消息，包括以下格式：</p>
<p><strong>请求行、请求头部、空行、请求数据</strong></p>
<h3 id="一个典型的HTTP请求示例"><a href="#一个典型的HTTP请求示例" class="headerlink" title="一个典型的HTTP请求示例"></a>一个典型的HTTP请求示例</h3><pre><code>GET https://www.baidu.com/ HTTP/1.1
Host: www.baidu.com
Connection: keep-alive
Upgrade-Insecure-Requests: 1
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8
Referer: http://www.baidu.com/
Accept-Encoding: gzip, deflate, sdch, br
Accept-Language: zh-CN,zh;q=0.8,en;q=0.6
Cookie: BAIDUID=04E4001F34EA74AD4601512DD3C41A7B:FG=1; BIDUPSID=04E4001F34EA74AD4601512DD3C41A7B; PSTM=1470329258; MCITY=-343%3A340%3A; BDUSS=nF0MVFiMTVLcUh-Q2MxQ0M3STZGQUZ4N2hBa1FFRkIzUDI3QlBCZjg5cFdOd1pZQVFBQUFBJCQAAAAAAAAAAAEAAADpLvgG0KGyvLrcyfrG-AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFaq3ldWqt5XN; H_PS_PSSID=1447_18240_21105_21386_21454_21409_21554; BD_UPN=12314753; sug=3; sugstore=0; ORIGIN=0; bdime=0; H_PS_645EC=7e2ad3QHl181NSPbFbd7PRUCE1LlufzxrcFmwYin0E6b%2BW8bbTMKHZbDP0g; BDSVRTM=0
</code></pre><h2 id="请求方法"><a href="#请求方法" class="headerlink" title="请求方法"></a><strong>请求方法</strong></h2><pre><code>GET https://www.baidu.com/ HTTP/1.1  
</code></pre><p>根据HTTP标准，HTTP请求可以使用多种请求方法。</p>
<p>HTTP 0.9：只有基本的文本 GET 功能。</p>
<p>HTTP 1.0：完善的请求/响应模型，并将协议补充完整，定义了三种请求方法： GET, POST 和 HEAD方法。</p>
<p>HTTP 1.1：在 1.0 基础上进行更新，新增了五种请求方法：OPTIONS, PUT, DELETE, TRACE 和 CONNECT 方法。</p>
<p>HTTP 2.0（未普及）：请求/响应首部的定义基本没有改变，只是所有首部键必须全部小写，而且请求行要独立为 :method、:scheme、:host、:path这些键值对。<br>|序号|方法|描述|<br>|—|:—:|—|<br>|1|GET|请求指定的页面信息，并返回实体主体。|<br>|2|HEAD|类似于get请求，只不过返回的响应中没有具体的内容，用于获取报头|<br>|3|POST|向指定资源提交数据进行处理请求（例如提交表单或者上传文件），数据被包含在请求体中。POST请求可能会导致新的资源的建立和/或已有资源的修改。|<br>|4|PUT|从客户端向服务器传送的数据取代指定的文档的内容。|<br>|5|DELETE|请求服务器删除指定的页面。|<br>|6|CONNECT|HTTP/1.1协议中预留给能够将连接改为管道方式的代理服务器。|<br>|7|OPTIONS|允许客户端查看服务器的性能。|<br>|8|TRACE|回显服务器收到的请求，主要用于测试或诊断。|</p>
<h3 id="HTTP请求主要分为Get和Post两种方法"><a href="#HTTP请求主要分为Get和Post两种方法" class="headerlink" title="HTTP请求主要分为Get和Post两种方法"></a><strong>HTTP请求主要分为Get和Post两种方法</strong></h3><ul>
<li><p>GET是从服务器上获取数据，POST是向服务器传送数据</p>
</li>
<li><p>GET请求参数显示，都显示在浏览器网址上，HTTP服务器根据该请求所包含URL中的参数来产生响应内容，即“Get”请求的参数是URL的一部分。 例如： <a href="http://www.baidu.com/s?wd=Chinese" target="_blank" rel="noopener">http://www.baidu.com/s?wd=Chinese</a></p>
</li>
<li><p>POST请求参数在请求体当中，消息长度没有限制而且以隐式的方式进行发送，通常用来向HTTP服务器提交量比较大的数据（比如请求中包含许多参数或者文件上传操作等），请求的参数包含在“Content-Type”消息头里，指明该消息体的媒体类型和编码，</p>
</li>
</ul>
<p><strong>注意：避免使用Get方式提交表单，因为有可能会导致安全问题。 比如说在登陆表单中用Get方式，用户输入的用户名和密码将在地址栏中暴露无遗。</strong></p>
<h3 id="常用的请求报头"><a href="#常用的请求报头" class="headerlink" title="常用的请求报头"></a><strong>常用的请求报头</strong></h3><h4 id="1-Host-主机和端口号"><a href="#1-Host-主机和端口号" class="headerlink" title="1. Host (主机和端口号)"></a><strong>1. Host (主机和端口号)</strong></h4><p>Host：对应网址URL中的Web名称和端口号，用于指定被请求资源的Internet主机和端口号，通常属于URL的一部分。</p>
<h4 id="2-Connection-链接类型"><a href="#2-Connection-链接类型" class="headerlink" title="2. Connection (链接类型)"></a><strong>2. Connection (链接类型)</strong></h4><p>Connection：表示客户端与服务连接类型</p>
<ol>
<li><p>Client 发起一个包含 Connection:keep-alive 的请求，HTTP/1.1使用 keep-alive 为默认值。</p>
</li>
<li><p>Server收到请求后：</p>
<ul>
<li>如果 Server 支持 keep-alive，回复一个包含 Connection:keep-alive 的响应，不关闭连接；</li>
<li>如果 Server 不支持 keep-alive，回复一个包含 Connection:close 的响应，关闭连接。</li>
</ul>
</li>
<li><p>如果client收到包含 Connection:keep-alive 的响应，向同一个连接发送下一个请求，直到一方主动关闭连接。</p>
</li>
</ol>
<p><strong>keep-alive在很多情况下能够重用连接，减少资源消耗，缩短响应时间，比如当浏览器需要多个文件时(比如一个HTML文件和相关的图形文件)，不需要每次都去请求建立连接。</strong></p>
<h4 id="3-Upgrade-Insecure-Requests-升级为HTTPS请求"><a href="#3-Upgrade-Insecure-Requests-升级为HTTPS请求" class="headerlink" title="3. Upgrade-Insecure-Requests (升级为HTTPS请求)"></a><strong>3. Upgrade-Insecure-Requests (升级为HTTPS请求)</strong></h4><p>Upgrade-Insecure-Requests：升级不安全的请求，意思是会在加载 http 资源时自动替换成 https 请求，让浏览器不再显示https页面中的http请求警报。<br><strong><em>HTTPS 是以安全为目标的 HTTP 通道，所以在 HTTPS 承载的页面上不允许出现 HTTP 请求，一旦出现就是提示或报错。</em></strong></p>
<h4 id="4-User-Agent-浏览器名称"><a href="#4-User-Agent-浏览器名称" class="headerlink" title="4. User-Agent (浏览器名称)"></a><strong>4. User-Agent (浏览器名称)</strong></h4><p>User-Agent：是客户浏览器的名称。</p>
<h4 id="5-Accept-传输文件类型"><a href="#5-Accept-传输文件类型" class="headerlink" title="5. Accept (传输文件类型)"></a><strong>5. Accept (传输文件类型)</strong></h4><p>Accept：指浏览器或其他客户端可以接受的MIME（Multipurpose Internet Mail Extensions（多用途互联网邮件扩展））文件类型，服务器可以根据它判断并返回适当的文件格式。</p>
<p><strong>举例</strong><br>Accept: <em>/</em>：表示什么都可以接收。</p>
<p>Accept：image/gif：表明客户端希望接受GIF图像格式的资源；</p>
<p>Accept：text/html：表明客户端希望接受html文本。</p>
<p>Accept: text/html, application/xhtml+xml;q=0.9, image/*;q=0.8：表示浏览器支持的 MIME 类型分别是 html文本、xhtml和xml文档、所有的图像格式资源。<br><strong><em>q是权重系数，范围 0 =&lt; q &lt;= 1，q 值越大，请求越倾向于获得其“;”之前的类型表示的内容。若没有指定q值，则默认为1，按从左到右排序顺序；若被赋值为0，则用于表示浏览器不接受此内容类型。  TEXT:用于标准化地表示的文本信息，文本消息可以是多种字符集和或者多种格式的；Application：用于传输应用程序数据或者二进制数据。</em></strong></p>
<h4 id="6-Referer-页面跳转处"><a href="#6-Referer-页面跳转处" class="headerlink" title="6. Referer (页面跳转处)"></a><strong>6. Referer (页面跳转处)</strong></h4><p>Referer：表明产生请求的网页来自于哪个URL，用户是从该 Referer页面访问到当前请求的页面。这个属性可以用来跟踪Web请求来自哪个页面，是从什么网站来的等。</p>
<p>有时候遇到下载某网站图片，需要对应的referer，否则无法下载图片，那是因为人家做了防盗链，原理就是根据referer去判断是否是本网站的地址，如果不是，则拒绝，如果是，就可以下载；</p>
<h4 id="7-Accept-Encoding（文件编解码格式）"><a href="#7-Accept-Encoding（文件编解码格式）" class="headerlink" title="7. Accept-Encoding（文件编解码格式）"></a><strong>7. Accept-Encoding（文件编解码格式）</strong></h4><p>Accept-Encoding：指出浏览器可以接受的编码方式。编码方式不同于文件格式，它是为了压缩文件并加速文件传递速度。浏览器在接收到Web响应之后先解码，然后再检查文件格式，许多情形下这可以减少大量的下载时间。<br><strong>举例：Accept-Encoding:gzip;q=1.0, identity; q=0.5, *;q=0</strong></p>
<p>如果有多个Encoding同时匹配, 按照q值顺序排列，本例中按顺序支持 gzip, identity压缩编码，支持gzip的浏览器会返回经过gzip编码的HTML页面。 <strong>如果请求消息中没有设置这个域服务器假定客户端对各种内容编码都可以接受。</strong></p>
<h4 id="8-Accept-Language（语言种类）"><a href="#8-Accept-Language（语言种类）" class="headerlink" title="8. Accept-Language（语言种类）"></a><strong>8. Accept-Language（语言种类）</strong></h4><p>Accept-Langeuage：指出浏览器可以接受的语言种类，如en或en-us指英语，zh或者zh-cn指中文，当服务器能够提供一种以上的语言版本时要用到。</p>
<h4 id="9-Accept-Charset（字符编码）"><a href="#9-Accept-Charset（字符编码）" class="headerlink" title="9. Accept-Charset（字符编码）"></a><strong>9. Accept-Charset（字符编码）</strong></h4><p>Accept-Charset：指出浏览器可以接受的字符编码。<br><strong>举例：Accept-Charset:iso-8859-1,gb2312,utf-8</strong></p>
<ul>
<li>ISO8859-1：通常叫做Latin-1。Latin-1包括了书写所有西方欧洲语言不可缺少的附加字符，英文浏览器的默认值是ISO-8859-1.</li>
<li>gb2312：标准简体中文字符集;</li>
<li>utf-8：UNICODE 的一种变长字符编码，可以解决多种语言文本显示问题，从而实现应用国际化和本地化。</li>
</ul>
<p><strong>如果在请求消息中没有设置这个域，缺省是任何字符集都可以接受。</strong></p>
<h4 id="10-Cookie-（Cookie）"><a href="#10-Cookie-（Cookie）" class="headerlink" title="10. Cookie （Cookie）"></a><strong>10. Cookie （Cookie）</strong></h4><p>Cookie：浏览器用这个属性向服务器发送Cookie。Cookie是在浏览器中寄存的小型数据体，它可以记载和服务器相关的用户信息，也可以用来实现会话功能。</p>
<h4 id="11-Content-Type-POST数据类型"><a href="#11-Content-Type-POST数据类型" class="headerlink" title="11. Content-Type (POST数据类型)"></a><strong>11. Content-Type (POST数据类型)</strong></h4><p>Content-Type：POST请求里用来表示的内容类型。<br><strong>举例：Content-Type = Text/XML; charset=gb2312：</strong></p>
<p>指明该请求的消息体中包含的是纯文本的XML类型的数据，字符编码采用“gb2312”。</p>
<h3 id="服务端HTTP响应"><a href="#服务端HTTP响应" class="headerlink" title="服务端HTTP响应"></a>服务端HTTP响应</h3><pre><code>HTTP响应也由四个部分组成，分别是： 状态行、消息报头、空行、响应正文
HTTP/1.1 200 OK
Server: Tengine
Connection: keep-alive
Date: Wed, 30 Nov 2016 07:58:21 GMT
Cache-Control: no-cache
Content-Type: text/html;charset=UTF-8
Keep-Alive: timeout=20
Vary: Accept-Encoding
Pragma: no-cache
X-NWS-LOG-UUID: bd27210a-24e5-4740-8f6c-25dbafa9c395
Content-Length: 180945

&lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD XHTML 1.0 Transitional//EN&quot; ....
</code></pre><h3 id="响应状态码"><a href="#响应状态码" class="headerlink" title="响应状态码"></a>响应状态码</h3><p>响应状态代码有三位数字组成，第一个数字定义了响应的类别，且有五种可能取值。</p>
<h4 id="常见状态码："><a href="#常见状态码：" class="headerlink" title="常见状态码："></a>常见状态码：</h4><blockquote>
<ul>
<li>100~199表示服务器成功接收部分请求，要求客户端继续提交其余请求才能完成整个处理过程。</li>
<li>200~299：表示服务器成功接收请求并已完成整个处理过程。常用200（OK 请求成功）。</li>
<li>300~399：为完成请求，客户需进一步细化请求。例如：请求的资源已经移动一个新地址、常用302（所请求的页面已经临时转移至新的url）、307和304（使用缓存资源）。</li>
<li>400~499：客户端的请求有错误，常用404（服务器无法找到被请求的页面）、403（服务器拒绝访问，权限不够）。</li>
<li>500~599：服务器端出现错误，常用500（请求未完成。服务器遇到不可预知的情况）。</li>
</ul>
</blockquote>
<h1 id="Cookie-和-Session："><a href="#Cookie-和-Session：" class="headerlink" title="Cookie 和 Session："></a>Cookie 和 Session：</h1><p>服务器和客户端的交互仅限于请求/响应过程，结束之后便断开，在下一次请求时，服务器会认为新的客户端。</p>
<p>为了维护他们之间的链接，让服务器知道这是前一个用户发送的请求，必须在一个地方保存客户端的信息。</p>
<p><strong>Cookie：</strong>通过在 客户端 记录的信息确定用户的身份。</p>
<p><strong>Session：</strong>通过在 服务器端 记录的信息确定用户的身份。</p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/01/20180501/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mr.Q">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dynamic-Soul">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/01/20180501/" itemprop="url">通用爬虫和聚焦爬虫</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-01T21:20:02+08:00">2018-05-01</time>
            

            
            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/爬虫/" itemprop="url" rel="index"><span itemprop="name">爬虫</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <hr>
<p>根据使用场景，网络爬虫可分为 <strong>通用爬虫</strong> 和<strong>聚焦爬虫</strong>两种</p>
<hr>
<h2 id="通用爬虫"><a href="#通用爬虫" class="headerlink" title="通用爬虫"></a>通用爬虫</h2><p>通用网络爬虫 是 捜索引擎抓取系统（Baidu、Google、Yahoo等）的重要组成部分。主要目的是将互联网上的网页下载到本地，形成一个互联网内容的镜像备份。</p>
<h3 id="1-通用搜索引擎（Search-Engine）工作原理"><a href="#1-通用搜索引擎（Search-Engine）工作原理" class="headerlink" title="1. 通用搜索引擎（Search Engine）工作原理"></a>1. 通用搜索引擎（Search Engine）工作原理</h3><p><strong>通用网络爬虫</strong> 从互联网中搜集网页，采集信息，这些网页信息用于为搜索引擎建立索引从而提供支持，它决定着整个引擎系统的内容是否丰富，信息是否即时，因此其性能的优劣直接影响着搜索引擎的效果。</p>
<h4 id="第一步：抓取网页"><a href="#第一步：抓取网页" class="headerlink" title="第一步：抓取网页"></a>第一步：抓取网页</h4><p>搜索引擎网络爬虫的基本工作流程如下：</p>
<pre><code>1.首先选取一部分的种子URL，将这些URL放入待抓取URL队列；

2.取出待抓取URL，解析DNS得到主机的IP，并将URL对应的网页下载下来，存储进已下载网页库中，并且将这些URL放进已抓取URL队列。

3.分析已抓取URL队列中的URL，分析其中的其他URL，并且将URL放入待抓取URL队列，从而进入下一个循环....
</code></pre><font color="gray">搜索引擎如何获取一个新网站的URL：</font><br><font color="gray">1. 新网站向搜索引擎主动提交网址：（如百度<a href="http://zhanzhang.baidu.com/linksubmit/url）" target="_blank" rel="noopener">http://zhanzhang.baidu.com/linksubmit/url）</a></font><br><font color="gray">2. 在其他网站上设置新网站外链（尽可能处于搜索引擎爬虫爬取范围）</font><br><font color="gray">3. 搜索引擎和DNS解析服务商(如DNSPod等）合作，新网站域名将被迅速抓取。</font>

<font color="gray">但是搜索引擎蜘蛛的爬行是被输入了一定的规则的，它需要遵从一些命令或文件的内容，如标注为nofollow的链接，或者是Robots协议。</font>

<pre><code>Robots协议（也叫爬虫协议、机器人协议等），全称是“网络爬虫排除标准”（Robots Exclusion Protocol），网站通过Robots协议告诉
搜索引擎哪些页面可以抓取，哪些页面不能抓取，例如：

淘宝网：https://www.taobao.com/robots.txt

腾讯网： http://www.qq.com/robots.txt
</code></pre><h4 id="第二步：数据存储"><a href="#第二步：数据存储" class="headerlink" title="第二步：数据存储"></a>第二步：数据存储</h4><p>搜索引擎通过爬虫爬取到的网页，将数据存入原始页面数据库。其中的页面数据与用户浏览器得到的HTML是完全一样的。</p>
<p>搜索引擎蜘蛛在抓取页面时，也做一定的重复内容检测，一旦遇到访问权重很低的网站上有大量抄袭、采集或者复制的内容，很可能就不再爬行。</p>
<h4 id="第三步：预处理"><a href="#第三步：预处理" class="headerlink" title="第三步：预处理"></a>第三步：预处理</h4><p>搜索引擎将爬虫抓取回来的页面，进行各种步骤的预处理。</p>
<ul>
<li>提取文字</li>
<li>中文分词</li>
<li>消除噪音（比如版权声明文字、导航条、广告等……）</li>
<li>索引处理</li>
<li>链接关系计算</li>
<li>特殊文件处理</li>
<li>….<br>除了HTML文件外，搜索引擎通常还能抓取和索引以文字为基础的多种文件类型，如 PDF、Word、WPS、XLS、PPT、TXT 文件等。我们在搜索结果中也经常会看到这些文件类型。</li>
</ul>
<p>但搜索引擎还不能处理图片、视频、Flash 这类非文字内容，也不能执行脚本和程序。</p>
<h4 id="第四步：提供检索服务，网站排名"><a href="#第四步：提供检索服务，网站排名" class="headerlink" title="第四步：提供检索服务，网站排名"></a>第四步：提供检索服务，网站排名</h4><p>搜索引擎在对信息进行组织和处理后，为用户提供关键字检索服务，将用户检索相关的信息展示给用户。</p>
<p>同时会根据页面的PageRank值（链接的访问量排名）来进行网站排名，这样Rank值高的网站在搜索结果中会排名较前，当然也可以直接使用 Money 购买搜索引擎网站排名，简单粗暴。</p>
<h3 id="但是，这些通用性搜索引擎也存在着一定的局限性："><a href="#但是，这些通用性搜索引擎也存在着一定的局限性：" class="headerlink" title="但是，这些通用性搜索引擎也存在着一定的局限性："></a>但是，这些通用性搜索引擎也存在着一定的局限性：</h3><pre><code>1.通用搜索引擎所返回的结果都是网页，而大多情况下，网页里90%的内容对用户来说都是无用的。

2.不同领域、不同背景的用户往往具有不同的检索目的和需求，搜索引擎无法提供针对具体某个用户的搜索结果。

3.万维网数据形式的丰富和网络技术的不断发展，图片、数据库、音频、视频多媒体等不同数据大量出现，通用搜索引擎对这些文件无能为力，不能很好地发现和获取。

4.通用搜索引擎大多提供基于关键字的检索，难以支持根据语义信息提出的查询，无法准确理解用户的具体需求。
</code></pre><h3 id="针对这些情况，聚焦爬虫技术得以广泛使用。"><a href="#针对这些情况，聚焦爬虫技术得以广泛使用。" class="headerlink" title="针对这些情况，聚焦爬虫技术得以广泛使用。"></a>针对这些情况，聚焦爬虫技术得以广泛使用。</h3><p>聚焦爬虫，是”面向特定主题需求”的一种网络爬虫程序，它与通用搜索引擎爬虫的区别在于： <em>聚焦爬虫在实施网页抓取时会对内容进行处理筛选，尽量保证只抓取与需求相关的网页信息。</em></p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/01/2018052922/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mr.Q">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dynamic-Soul">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/01/2018052922/" itemprop="url">图片下载器爬虫</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-01T17:53:31+08:00">2018-05-01</time>
            

            
            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/爬虫/" itemprop="url" rel="index"><span itemprop="name">爬虫</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="items-py"><a href="#items-py" class="headerlink" title="items.py"></a>items.py</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CoserItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    url = scrapy.Field()</span><br><span class="line">    name = scrapy.Field()</span><br><span class="line">    info = scrapy.Field()</span><br><span class="line">    image_urls = scrapy.Field()</span><br><span class="line">    images = scrapy.Field()</span><br></pre></td></tr></table></figure>
<h2 id="spiders-coser-py"><a href="#spiders-coser-py" class="headerlink" title="spiders/coser.py"></a>spiders/coser.py</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> scrapy.selector <span class="keyword">import</span> Selector</span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.contrib.loader <span class="keyword">import</span> ItemLoader</span><br><span class="line"><span class="keyword">from</span> Cosplay.items <span class="keyword">import</span> CoserItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CoserSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"coser"</span></span><br><span class="line">    allowed_domains = [<span class="string">"bcy.net"</span>]</span><br><span class="line">    start_urls = (</span><br><span class="line">        <span class="string">'http://bcy.net/cn125101'</span>,</span><br><span class="line">        <span class="string">'http://bcy.net/cn126487'</span>,</span><br><span class="line">        <span class="string">'http://bcy.net/cn126173'</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        sel = Selector(response)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> link <span class="keyword">in</span> sel.xpath(<span class="string">"//ul[@class='js-articles l-works']/li[@class='l-work--big']/article[@class='work work--second-created']/h2[@class='work__title']/a/@href"</span>).extract():</span><br><span class="line">            link = <span class="string">'http://bcy.net%s'</span> % link</span><br><span class="line">            request = scrapy.Request(link, callback=self.parse_item)</span><br><span class="line">            <span class="keyword">yield</span> request</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_item</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        l = ItemLoader(item=CoserItem(), response=response)</span><br><span class="line">        l.add_xpath(<span class="string">'name'</span>, <span class="string">"//h1[@class='js-post-title']/text()"</span>)</span><br><span class="line">        l.add_xpath(<span class="string">'info'</span>, <span class="string">"//div[@class='post__info']/div[@class='post__type post__info-group']/span/text()"</span>)</span><br><span class="line">        urls = l.get_xpath(<span class="string">'//img[@class="detail_std detail_clickable"]/@src'</span>)</span><br><span class="line">        urls = [url.replace(<span class="string">'/w650'</span>, <span class="string">''</span>) <span class="keyword">for</span> url <span class="keyword">in</span> urls]</span><br><span class="line">        l.add_value(<span class="string">'image_urls'</span>, urls)</span><br><span class="line">        l.add_value(<span class="string">'url'</span>, response.url)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> l.load_item()</span><br></pre></td></tr></table></figure>
<h2 id="pipelines-py"><a href="#pipelines-py" class="headerlink" title="pipelines.py"></a>pipelines.py</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> Cosplay <span class="keyword">import</span> settings</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ImageDownloadPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">'image_urls'</span> <span class="keyword">in</span> item:</span><br><span class="line">            images = []</span><br><span class="line">            dir_path = <span class="string">'%s/%s'</span> % (settings.IMAGES_STORE, spider.name)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(dir_path):</span><br><span class="line">                os.makedirs(dir_path)</span><br><span class="line">            <span class="keyword">for</span> image_url <span class="keyword">in</span> item[<span class="string">'image_urls'</span>]:</span><br><span class="line">                us = image_url.split(<span class="string">'/'</span>)[<span class="number">3</span>:]</span><br><span class="line">                image_file_name = <span class="string">'_'</span>.join(us)</span><br><span class="line">                file_path = <span class="string">'%s/%s'</span> % (dir_path, image_file_name)</span><br><span class="line">                images.append(file_path)</span><br><span class="line">                <span class="keyword">if</span> os.path.exists(file_path):</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">with</span> open(file_path, <span class="string">'wb'</span>) <span class="keyword">as</span> handle:</span><br><span class="line">                    response = requests.get(image_url, stream=<span class="keyword">True</span>)</span><br><span class="line">                    <span class="keyword">for</span> block <span class="keyword">in</span> response.iter_content(<span class="number">1024</span>):</span><br><span class="line">                        <span class="keyword">if</span> <span class="keyword">not</span> block:</span><br><span class="line">                            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">                        handle.write(block)</span><br><span class="line"></span><br><span class="line">            item[<span class="string">'images'</span>] = images</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>
<h2 id="settings-py"><a href="#settings-py" class="headerlink" title="settings.py"></a>settings.py</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;<span class="string">'Cosplay.pipelines.ImageDownloadPipeline'</span>: <span class="number">1</span>&#125;</span><br><span class="line"></span><br><span class="line">IMAGES_STORE = <span class="string">'../Images'</span></span><br><span class="line"></span><br><span class="line">DOWNLOAD_DELAY = <span class="number">0.25</span>    <span class="comment"># 250 ms of delay</span></span><br></pre></td></tr></table></figure>
<h2 id="在项目根目录下新建main-py文件-用于调试"><a href="#在项目根目录下新建main-py文件-用于调试" class="headerlink" title="在项目根目录下新建main.py文件,用于调试"></a>在项目根目录下新建main.py文件,用于调试</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> cmdline</span><br><span class="line">cmdline.execute(<span class="string">'scrapy crawl coser'</span>.split())</span><br></pre></td></tr></table></figure>
<h2 id="执行程序"><a href="#执行程序" class="headerlink" title="执行程序"></a>执行程序</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">py2 main.py</span><br></pre></td></tr></table></figure>
          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/03/18/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mr.Q">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dynamic-Soul">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/03/18/hello-world/" itemprop="url">爬虫基本内容</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-03-18T18:44:12+08:00">2018-03-18</time>
            

            
            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/爬虫/" itemprop="url" rel="index"><span itemprop="name">爬虫</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="1-什么是爬虫？"><a href="#1-什么是爬虫？" class="headerlink" title="1 . 什么是爬虫？"></a>1 . 什么是爬虫？</h3><p>爬虫：就是抓取网页数据的程序</p>
<h3 id="2-爬虫怎么抓取网页数据？"><a href="#2-爬虫怎么抓取网页数据？" class="headerlink" title="2 . 爬虫怎么抓取网页数据？"></a>2 . 爬虫怎么抓取网页数据？</h3><p>首先来看网页的三个特征：   </p>
<font color="#A52A2A" face="黑体">①每个网页都有自己唯一的的URL（统一资源定位符）来进行定位；<br>②网页使用HTML（超文本标记语言）来描述页面信息；<br>③网页使用HTTP/HTTPS（超文本传输协议）来传输HTML数据。</font>  

<p>爬虫的设计思路：  </p>
<p><font color="#A52A2A" face="黑体">①首先确定需要爬取网页的URL地址；<br>②通过HTTP/HTTPS协议来获取相应的HTML页面；<br>③提取HTML页面里有用的数据<br>&nbsp;&nbsp;a.如果是需要的数据，保存；<br>&nbsp;&nbsp;b.如果是页面里的其他URL，那就继续执行第②步。</font>  </p>
<h3 id="3-为什么选择Python语言写爬虫？"><a href="#3-为什么选择Python语言写爬虫？" class="headerlink" title="3.为什么选择Python语言写爬虫？"></a>3.为什么选择Python语言写爬虫？</h3><p>&nbsp;&nbsp;可以写爬虫的语言很多，如PHP、Java、C/C++、Python等等…  </p>
<p>&nbsp;&nbsp;①PHP虽然是世界上最好的语言（滑稽），但是它天生不是干这个的，而且对多线程、异步支持不够好，并发处理能力很弱。<br>&nbsp;&nbsp;爬虫是工具性程序，对速度和效率要求比较高。  </p>
<p>&nbsp;&nbsp;②Java的爬虫生态圈也很晚上，是Python爬虫最大的对手。但是Java语言本身笨重，代码量大。重构成本较高，任何修改都会导致代码的大量改动。<br>&nbsp;&nbsp;爬虫经常需要修改部分采集代码。  </p>
<p>&nbsp;&nbsp;③C/C++运行效率和性能几乎最强，但是学习成本很高。代码成型比较慢。<br>&nbsp;&nbsp;能用C/C++写爬虫，是能力的表现，但不是最好的选择。  </p>
<p>&nbsp;&nbsp;④Python语法优美，代码简洁，开发效率高，支持的模块多，相关的HTTP请求模块和HTML解析模块非常丰富。<br>&nbsp;&nbsp;还有强大的爬虫Scrapy，以及成熟高效的Scrapy—redis分布式策略。而且，调用其他接口也非常方便（胶水语言）。</p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/7/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><span class="page-number current">8</span>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.gif"
                alt="Mr.Q" />
            
              <p class="site-author-name" itemprop="name">Mr.Q</p>
              <p class="site-description motion-element" itemprop="description">我希望时间没有流走，世界还停在过去，昏黄的光还照在旧日的路上，我刚刚走过，恰与你们相识。</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">76</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">4</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">16</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          
          

          
          

          
            
          
          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Mr.Q</span>

  

  
</div>




  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/theme-next/hexo-theme-next">NexT.Gemini</a> v6.0.6</div>




<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共112.7k字</span>
</div>
        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>














  













  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.0.6"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.0.6"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=6.0.6"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=6.0.6"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.0.6"></script>



  



	





  





  










  





  

  

  

  

  
  

  

  

  

  


  
</body>
</html>
